{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the documentation website for the BioCloud HPC cluster at the Section of Chemistry and Biotechnology at Aalborg University. Here, you\u2019ll find everything you need to get started with accessing and using the cluster for scientific computing. The system is managed by the SLURM workload manager, which efficiently handles job scheduling and resource allocation for hundreds of simultaneous users through isolated computing jobs.</p> <p>The current cluster consists of data center-grade servers equipped with dual-socket AMD EPYC CPUs of various sizes and generations. These servers have been contributed by individual research groups through their own research grants. In total, the cluster provides 3872 CPU cores and 23 TB of shared memory, all connected to a 2 PB CephFS networked storage cluster.</p> <p>To gain access, you must first contact an administrator to grant access to your AAU user account. Then, please read everything on this website before starting to use the cluster. There is nothing here you don't need to be familiar with!</p>"},{"location":"access/ssh/","title":"Shell access through SSH","text":"<p>SSH (Secure Shell) is a widely used protocol for securely gaining access to a shell (or terminal) on remote Linux machines and is the primary way to access the BioCloud servers. This page provides instructions on how to access the BioCloud through SSH using a few different SSH clients on all major platforms. There are many other SSH clients available than those listed below, but regardless of the client, everything will run over the same SSH protocol in the exact same way.</p> <p>If you need to run interactive GUI (graphical user interface) apps like CLC, Arb, RStudio, VS Code, etc, you don't necessarily need to connect through SSH. Instead you can use the interactive web portal described on the next page.</p> <p>BioCloud is only available while connected to the local campus network. To connect from elsewhere you first need to either connect to the university VPN, or configure SSH to connect through a public SSH gateway, see external access below. You then authenticate using your usual AAU account credentials.</p> <p>The hostnames of the login nodes are:</p> <ul> <li><code>bio-fe01.srv.aau.dk</code></li> <li><code>bio-fe02.srv.aau.dk</code></li> </ul> <p>After successfully logging in to one of the login nodes using any of the SSH clients mentioned below, please consult and carefully read the Resource management section to learn how to submit computing jobs correctly from a login node.</p> <p>Login nodes are not for computing workloads</p> <p>It is strictly forbidden to run any computing workloads on the login nodes. Everything must be submitted to the dedicated compute nodes through SLURM batch jobs - regardless of their size. The only purpose of the login nodes is to provide an access point from which you can submit computing jobs to the (much more powerful) dedicated compute nodes, transfer files, edit scripts, and install software. With many concurrent users things add up quickly, and running workloads on the login nodes can easily lead to both performance- and connection problems for everyone.</p>"},{"location":"access/ssh/#code-editors-ides","title":"Code editors (IDEs)","text":"<p>It's rarely enough with just a terminal because you more often than not need to edit some scripts in order to do anything, which is not very convenient to do in a terminal, so below are some instructions on how to connect using a few popular code editors, or IDEs (integrated development environments), with built-in SSH support, but also just a terminal.</p>"},{"location":"access/ssh/#visual-studio-code","title":"Visual Studio Code","text":"<p>Visual Studio Code (VS Code) is a popular free cross-platform code editor with a myriad of extensions available for anything and everything including syntax highlighting for any programming language, integrated git support, GitHub copilot for AI autocompletion of code, and the list goes on. If you want one editor for everything, there isn't currently anything better out there. If you need an interactive VS Code session and actually run things from there, please follow this guide to connect VS Code directly to a SLURM job instead. Alternatively, you can also start a Code Server (an open source alternative to VS Code) in a SLURM job from the interactive web portal described on the next page.</p>"},{"location":"access/ssh/#installation-windows-macos-or-linux","title":"Installation (Windows, macOS, or Linux)","text":"<p>Download and install using the instructions on the official website.</p>"},{"location":"access/ssh/#connecting-to-a-login-node","title":"Connecting to a login-node","text":"<ol> <li>Open VS Code and install the \"Remote - SSH\" extension from the Extensions sidebar menu.</li> <li>Click on the \"Remote Explorer\" icon after the extension has been installed.</li> <li>Add a host by either:<ul> <li>clicking on the \"+\" icon and enter your AAU email followed by <code>@</code> and then the server's hostname, for example: <code>abc@bio.aau.dk@bio-fe01.srv.aau.dk</code>, or</li> <li>add all servers at once using the SSH config template provided below by clicking the gear icon \"Open SSH Config File\" and paste its contents (optional). </li> </ul> </li> <li>Connect and log in with your SSH password.</li> <li>Once connected open a project or workspace folder (or create one while doing so) by clicking File -&gt; Open Folder (CTRL+k CTRL+o, CMD instead of CTRL if on macOS) to start your work</li> </ol> <p>Project folders in VSCode</p> <p>When connecting using VSCode, please only open individual project folders and not your entire home folder. If you do VSCode may scan all files in your home folder, which can leave a forever-running process on the login-node consuming all the CPUs available. Instead, it's much better to use workspaces and only add relevant folders one-by-one, if you need to work within multiple folders at once:</p> <p></p>"},{"location":"access/ssh/#mobaxterm","title":"MobaXterm","text":"<p>A more old-school and Windows-only client is MobaXterm. It's not as full-featured as VS Code, but is more lightweight.</p>"},{"location":"access/ssh/#installation-windows-only","title":"Installation (Windows only)","text":"<p>Download and install MobaXterm Home Edition from the official website.</p>"},{"location":"access/ssh/#connecting-to-a-server","title":"Connecting to a server","text":"<ol> <li>Open MobaXterm and click on the \"Start local terminal\" button.</li> <li>In the terminal, use the <code>ssh</code> command to connect to a server by entering AAU email followed by <code>@</code> and then the server's hostname, for example: <code>ssh abc@bio.aau.dk@bio-fe01.srv.aau.dk</code>.</li> </ol> <p>If you add the servers to your SSH config file the server hostnames should auto-complete.</p>"},{"location":"access/ssh/#just-a-terminal","title":"Just a terminal","text":"<p>For a simple terminal you can on Windows use for example PuTTY or msys2, and the integrated terminal and the <code>ssh</code> command itself directly if on Linux or macOS (also Windows). Type <code>ssh email@hostname</code> like above to connect immediately or just the hostname if you've added them to the SSH config file.</p>"},{"location":"access/ssh/#transferring-files","title":"Transferring files","text":"<p>Both VS Code and MobaXterm support file transfers, but you can also use other GUI apps like FileZilla or WinSCP. When just using a terminal there are several tools like <code>scp</code>, <code>rsync</code>, <code>rclone</code>, or <code>sftp</code>, all of which connect through the SSH protocol. You can also browse and transfer smaller files through the interactive web portal described on the next page.</p>"},{"location":"access/ssh/#external-access","title":"External access","text":"<p>To access the servers while not directly connected to any AAU or eduroam network there are two options. Either you connect through VPN, which will route all your traffic through AAU, or you can use the SSH gateway through <code>sshgw.aau.dk</code> for SSH connections only. If you need virtual desktop access only VPN will work (for now).</p>"},{"location":"access/ssh/#vpn","title":"VPN","text":"<p>The simplest way is to connect to the AAU VPN using the guide provided by ITS here. After you connect everything should work as though you were at AAU.</p>"},{"location":"access/ssh/#using-an-ssh-jump-host","title":"Using an SSH jump host","text":"<p>The SSH gateway is simply a server hosted at AAU whose only purpose is to bridge SSH connections from the outside (open port 22 ingress) to something else on the internal AAU network. Setting up 2-factor authentication is required in order to connect. To enable the \"proxy jumping\" you need to adjust the SSH config file by uncommenting the <code>ProxyJump sshgw.aau.dk</code> line under the <code>*.srv.aau.dk</code> host, see the SSH config template file. Keep in mind that as long as it's enabled it will always connect through <code>sshgw.aau.dk</code> regardless of which network you are connected to, including the AAU network, which is not always ideal.</p>"},{"location":"access/ssh/#additional-configuration-optional","title":"Additional configuration (optional)","text":""},{"location":"access/ssh/#ssh-config-file","title":"SSH config file","text":"<p>To avoid typing hostnames and user names constantly here's a template SSH config file that includes all current servers. The file must be saved at certain locations depending on your OS. On Windows it's here, and on macOS and Linux it's usually under <code>~/.ssh/config</code>. Hostnames in the file will then be auto-completed and if you've set up SSH public key authentication you won't need to type anything else than for example <code>ssh bio-fe01.srv.aau.dk</code> and you're in.</p>"},{"location":"access/ssh/#ssh-config-file-template","title":"SSH config file template","text":"<pre><code># nice-to-have global options that apply to all servers\n# prevents disconnects on network glitches and wifi reconnects,\n# allows forwarding GUI apps to the client desktop etc\nHost *\n    ServerAliveInterval 60\n    ServerAliveCountMax 2\n    ExitOnForwardFailure yes\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\n# Options for all BioCloud hosts, e.g. use the same user name (and optionally SSH key) everywhere\nHost bio-* bio-*.srv.aau.dk sshgw.aau.dk\n    User abc@bio.aau.dk\n    Port 22\n    IdentityFile ~/.ssh/biocloud\n    ForwardAgent yes\n\n# BioCloud login nodes\n# uncomment the ProxyJump line to enable connecting through sshgw.aau.dk for external access (to avoid using VPN)\nHost bio-fe*\n    #ProxyJump sshgw.aau.dk\nHost bio-fe01\n    HostName bio-fe01.srv.aau.dk\nHost bio-fe02\n    HostName bio-fe02.srv.aau.dk\n\n# authenticate with GitHub using an SSH key if needed (check guides section)\nHost github.com\n    HostName github.com\n    User git\n    Port 22\n    IdentityFile ~/.ssh/github\n</code></pre>"},{"location":"access/ssh/#ssh-public-key-authentication","title":"SSH Public Key Authentication","text":"<p>SSH public key authentication offers a more secure way to connect to a server, and is also more convenient, since you don't have to type in your password every single time you connect through SSH. This is possible by generating a cryptographically linked key pair consisting of a private- and a public key file, where only the public key can be generated from the private key, but NOT the other way around. The private key can then be used to authenticate with a server holding the cryptographically linked public key for your user (think of the public key as the lock for the matching private key). You can even add an additional layer of security by encrypting the private key itself using a password when generating the pair. Any SSH client that you choose to use will connect through the SSH program on your computer under the hood, so public key authentication will also apply to them if set up like below. </p> <p>If you also want to be able to connect to GitHub from the terminal through SSH, password authentication is not even allowed - you MUST use public key authentication. A separate guide for that is available under the guides section here.</p>"},{"location":"access/ssh/#generating-ssh-key-pairs","title":"Generating SSH Key Pairs","text":"<p>Generating the key pair must be done locally for security reasons, so that the private key never leaves your computer. If you use a password manager (please do) like 1Password or bitwarden, you can usually both generate and safely store and use SSH keys directly from your vault to avoid having private key files lying around on your computer. It's important that the key is not generated using the (usually) default RSA type algorithm, because it's outdated and can easily be brute-forced with modern hardware, so use for example the <code>ed25519</code> algorithm instead.</p> <ol> <li>Open your terminal (on Windows called command prompt, hit <code>win+r</code> and type <code>cmd</code>)</li> <li>Generate an SSH key pair by running the command: <code>ssh-keygen -t ed25519</code></li> <li>Follow the prompts, and save the two keyfiles somewhere safe (the convention is to place it somewhere in the hidden folder <code>~/.ssh/</code> in your home folder)</li> </ol>"},{"location":"access/ssh/#adding-public-keys-to-the-server","title":"Adding Public Keys to the Server","text":"<p>Copy your public key to the server using <code>ssh-copy-id -i ~/.ssh/biocloud.pub username@hostname</code>, or manually add the contents of the public key file into the <code>~/.ssh/authorized_keys</code> file on any of the servers (the home folder is shared across them all). SSH requires that your user is the only one able to read the contents before you are allowed to login, so ensure the correct permissions are set using <code>chmod 700 ~/.ssh &amp;&amp; chmod 600 ~/.ssh/authorized_keys</code>.</p> <p>You should now be able to login securely without typing a password - have fun!</p>"},{"location":"access/webportal/","title":"Interactive Web portal (OpenOndemand)","text":"<p>Another way to access the BioCloud compute resources is to use the interactive web portal based on Open Ondemand developed by Ohio Supercomputer Center. The main purpose of the web portal is to greatly simplify running interactive and graphical apps such as virtual desktops and IDEs, which are served directly from isolated SLURM jobs running on the compute nodes, but you can also easily browse around and transfer files, obtain a shell, check the job queue, or compose job batch scripts based on templates, and more - all from your browser without having to learn a multitude of terminal commands.</p> <p>This page only describes briefly how to connect to the web portal. Guides detailing how to use all of its features are available under the Guides section. It is also absolutely essential to read and understand how SLURM and resource management works (described on the next page), before using any of the apps, because interactive apps are often terribly inefficient (i.e. CPUs do nothing when you are just typing or clicking around)!</p>"},{"location":"access/webportal/#getting-access","title":"Getting access","text":"<p>In order to access the web portal you must first be connected to the local AAU campus network, or through VPN. Then go to https://biocloud.bio.aau.dk and log in using your usual AAU credentials if you aren't already (for example if you've already signed in to your AAU webmail, etc).</p> <p>If this is the first time you ever log in to any of the BioCloud resources, your home directory does not yet exist on the storage and you will likely see an error. To create your home directory you must log in just once either through SSH as described on the previous page or using the web portal shell, which is available here. You can also just contact an admin. Now you should be greeted by the front page:</p> <p></p> <p>Now, please read through the SLURM guide on the following pages before using any of the apps. Afterwards you can go through the guides for the individual apps under the Guides section.</p>"},{"location":"guides/git/","title":"Connecting to GitHub","text":"<p>This guide describes how to connect and authenticate with GitHub through SSH. It's a boiled down version of the official GitHub guide Connecting to GitHub with SSH covering only basic functionality, so if you need additional features go read that instead.</p>"},{"location":"guides/git/#connecting-from-vs-code","title":"Connecting from VS Code","text":"<p>VS Code has a fantastic builtin source control feature with everything you need. If you only need to use Git in VS Code, you don't need to do much else than just sign in with GitHub by clicking on the \"Accounts\" icon in the bottom left corner:</p> <p></p> <p>This will also synchronize all your settings across devices. Note that authenticating with GitHub in this way is not possible when using the Code Server app, in which case you must read on.</p>"},{"location":"guides/git/#connecting-through-ssh","title":"Connecting through SSH","text":"<p>To be able to commit and push changes to GitHub repositories (or to pull private repositories) through the git command line, or from practically any application with git support, you must connect through SSH using public key authentication very similarly to how you would connect to a login node through SSH using a key-pair. GitHub simply doesn't support password authentication for SSH access anymore because it's very insecure. The only alternative to SSH keys is personal access tokens, but they are more suited for fine-grained control within large organizations and non-interactive API integrations etc.</p>"},{"location":"guides/git/#generate-an-ssh-key-pair","title":"Generate an SSH key pair","text":"<p>Start by generating a key pair using the below commands. The email is not strictly necessary, but can help you differentiate multiple keys. You will also be asked whether you want to encrypt the private key file using a password, which is recommended for extra security, especially when the private key file will be located on a shared filesystem. If you choose to do so it can be handy to also add the key to the SSH agent to avoid having to type the password every single time you want to commit, push, and pull etc. For windows users, however, it can be troublesome, so you can skip this step and instead rely on proper permissions ensuring that you are the only user with read access to the file:</p> <pre><code>mkdir -p ~/.ssh\nchmod 700 ~/.ssh\nssh-keygen -t ed25519 -C \"your_email@example.com\" -f ~/.ssh/github\nchmod 400 ~/.ssh/github\n</code></pre>"},{"location":"guides/git/#add-the-public-key-to-github","title":"Add the public key to GitHub","text":"<p>Now you must add the public key file on GitHub to allow access (think of the public key as \"the lock\", and the private key \"the key\" to it). Copy the contents of the public key file using a terminal editor, fx <code>nano ~/.ssh/github.pub</code> (use <code>CTRL/CMD+SHIFT+C</code>, and NOT <code>CTRL/CMD+C</code>, which in a terminal means interrupt). Then sign in to github.com, go to settings by clicking your profile picture in the top right corner, and find \"SSH and GPG keys\" in the menus under \"Access\":</p> <p></p> <p>Click \"New SSH key\":</p> <p></p> <p>Paste the contents of the public key file and click \"Add SSH key\":</p> <p></p> <p>The email will be removed automatically. If you now go to <code>http://github.com/yourusername.keys</code>, you should see the public key listed there.</p>"},{"location":"guides/git/#configure-ssh","title":"Configure SSH","text":"<p>As the last step, you need to configure SSH to automatically use the new private key when connecting to GitHub by adding the following to the <code>~/.ssh/config</code> file. If you've used the SSH config template file provided in the Shell access through SSH guide, it will already be there, so you can skip this step: <pre><code># authenticate with GitHub using an SSH key if present\nHost github.com\n    HostName github.com\n    User git\n    Port 22\n    IdentityFile ~/.ssh/github\n</code></pre></p>"},{"location":"guides/git/#test-the-connection","title":"Test the connection","text":"<p>Now test the connection by running <code>ssh -T github.com</code>. If you see the below message, it works! <pre><code>$ ssh -T github.com\nHi USERNAME! You've successfully authenticated, but GitHub does not provide shell access.\n</code></pre></p>"},{"location":"guides/sra/","title":"Downloading data from NCBI SRA","text":"<p>https://bioinformatics.ccr.cancer.gov/docs/b4b/Module1_Unix_Biowulf/Lesson6/</p>"},{"location":"guides/sra/#download-sra-tools-container","title":"Download sra-tools container","text":"<pre><code>singularity pull docker://ncbi/sra-tools\n</code></pre>"},{"location":"guides/sra/#get-data","title":"Get data","text":"<p>bioproject: PRJNA192924 sra: SRR1154613 prefetch first, then use fasterq-dump <pre><code>singularity run sra-tools_latest.sif prefetch SRR1154613\nsingularity run sra-tools_latest.sif fasterq-dump --threads $(nproc) --progress --split-3 SRR1154613\n</code></pre></p>"},{"location":"guides/sra/#download-sra-data-from-ena-instead","title":"Download SRA data from ENA instead","text":"<p>https://www.ebi.ac.uk/ena/browser/view/SRR1154613</p>"},{"location":"guides/sshdslurm/","title":"Interactive VS Code session in a SLURM job","text":"<p>When using the Visual Studio Code IDE to work remotely, you would normally install the Remote - SSH on your local computer and connect to the particular machine over an SSH connection to work there instead. This is useful when you for example need to work closer to raw data to avoid moving large data around, or when you simply need more juice for demanding tasks. In order to use VS Code remotely on a SLURM cluster, however, you can't normally connect directly to a compute node, and connecting directly to login nodes to run things there is strictly forbidden. This small guide will show you how you can connect your VS Code session directly to a SLURM job running on a compute node instead.</p> Consider using Code Server from the web interface <p>Before continuing with this guide, first consider if using a Code Server from the web interface is sufficient for your needs instead, as it is much easier to get started with. Code Server is an open source alternative to Visual Studio Code provided by Coder, with almost the same features. If you really need the full VS Code IDE experience, however, then continue reading.</p>"},{"location":"guides/sshdslurm/#how-does-it-work","title":"How does it work?","text":"<p>When you normally connect to a remote server, you use an SSH client to connect to the SSH daemon <code>sshd</code>, which is a service running in the background on the server. You then simply talk to this daemon using the SSH protocol to execute commands on the server. The trick here is then to start a separate <code>sshd</code> process that runs within a SLURM job to connect to instead through a bridge connection to the job through one of the login nodes.</p>"},{"location":"guides/sshdslurm/#slurm-job-script","title":"SLURM job script","text":"<p>Log in through SSH on one of the login nodes, copy the following SLURM batch script somewhere, and adjust the resource requirements for your session. Submit the job using <code>sbatch</code> as usual, and remember to cancel it when you are done. It won't stop when you close the VS Code window on your computer unless it runs out of time. You will have to submit a job like this every time you want to use VS Code interactively (for anything else than code editing and file browsing etc). </p> <pre><code>#!/bin/bash\n#SBATCH --output=/dev/null\n#SBATCH --job-name=sshdbridge\n#SBATCH --time=0-9:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=3G\n\n# exit on first error or unset variable\nset -eu\n\n# find open port (open and close a socket, and reuse the port)\nPORT=$(python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\nscontrol update JobId=\"$SLURM_JOB_ID\" Comment=\"$PORT\"\n\n# check whether SSH host key exists (used for sshd host authentication, NOT for user authentication)\nssh_host_key=\"ssh_host_ed25519_key\"\nif [[ ! -s \"${HOME}/.ssh/${ssh_host_key}\" ]]\nthen\n  mkdir -p \"${HOME}/.ssh/\"\n  ssh-keygen -t ed25519 -N \"\" -f \"${HOME}/.ssh/${ssh_host_key}\"\n  chmod 600 \"${HOME}/.ssh/${ssh_host_key}\"\nfi\n\n# start sshd server on the available port\necho \"Starting sshd on port $PORT\"\n/usr/sbin/sshd -D -p \"${PORT}\" -o \"UsePAM no\" -h \"${HOME}/.ssh/ssh_host_ed25519_key\"\n</code></pre> Public key authentication is required <p>In order to connect to the job, you must set up and use public key authentication, password authentication will not work.</p>"},{"location":"guides/sshdslurm/#configure-ssh-connection","title":"Configure SSH connection","text":"<p>On your local computer, you need to set up a bridge connection through a login node by adding a few lines to your SSH config file. You will only have to do this once. You can find it through VS Code under the \"Remote Explorer\" side menu by clicking the little cog \"Open SSH Config File\":</p> <p></p> <p>Then add the following line somewhere:</p> <p>Windows <pre><code>Host bio-fe02-sshdbridge\n    ProxyCommand ssh bio-fe02.srv.aau.dk bash -c \\\"nc \\$(squeue --me --name=sshdbridge --states=R -h -O NodeList,Comment)\\\"\n</code></pre></p> <p>Linux/MacOS <pre><code>Host bio-fe02-sshdbridge\n    ProxyCommand ssh bio-fe02.srv.aau.dk \"nc \\$(squeue --me --name=sshdbridge --states=R -h -O NodeList,Comment)\"\n</code></pre></p> <p>In this example, the hostname of the login node <code>bio-fe02.srv.aau.dk</code> must already exist in your SSH config file for it to work. You can use the provided SSH config template if you haven't added any BioCloud hosts there yet, and you can also use any other login node. Save the file and you should now see the new bridge host under \"Remote Explorer\" (you may need to hit refresh first):</p> <p></p> <p>Finally, click the \"Connect in New (or current) Window\" icon next to the name. If all goes well you should see the name of the login node (as it's named in your SSH config) in the bottom left corner:</p> <p></p> <p>Now you can start working! Whatever you do in VS Code now will run remotely inside a SLURM job on one of the compute nodes with a connection through the login node. Magic.</p>"},{"location":"guides/sshdslurm/#notes","title":"Notes","text":"<ul> <li>You will NOT be able to connect if you use an SSH jump host. Connect through VPN instead if you are not at AAU.</li> <li>You can have multiple simultaneous connections to the same job, however if there are different resource requirements for each task you need to do, you must submit individual jobs and use a different name for each job, and also create separate entries in the SSH config for each job.</li> </ul>"},{"location":"guides/snakemake/biocloud/","title":"Running Snakemake workflows on SLURM clusters","text":"<p>Please first ensure you understand the basics of submitting SLURM jobs before running Snakemake workflows (or anything else!) on the BioCloud. It's highly recommended that you use a profile like the one provided below to properly allow Snakemake to start tasks as individual SLURM jobs and not run Snakemake itself in a large resource allocation (=job). Snakemake itself hardly requires any resources, 1CPU and 1GB memory is plenty. It's always important when developing Snakemake workflows to make sure that reasonable resource requirements are defined for the individual rules in the workflow (listed under <code>resources:</code> and <code>threads:</code>). Then it's only a matter of letting Snakemake be aware that it's being used on a HPC cluster and it will do things properly for you.</p>"},{"location":"guides/snakemake/biocloud/#dry-run-for-inspection","title":"Dry run for inspection","text":"<p>Before running the workflow, the DAG visualization mentioned on the previous page is a very useful way to quickly get an overview of exactly which tasks will be run and the dependencies between them. It can become quite large though, so it can also be useful to perform a \"dry run\", where Snakemake will output all of the tasks to be run without actually running anything. This can be done in a small interactive job and the output piped to a file with fx:</p> <pre><code>srun --ntasks 1 --cpus-per-task 1 --mem 1G snakemake -n &gt; workflow_dryrun.txt\n</code></pre> <p>If you have used the recommended folder structure mentioned earlier, Snakemake will automatically detect and use the <code>workflow/Snakefile</code>, but you may have more than one, in which case you must also supply <code>-s &lt;path to Snakefile&gt;</code>. </p>"},{"location":"guides/snakemake/biocloud/#submit-the-workflow","title":"Submit the workflow","text":"<p>When you have inspected the DAG or output from the dry run and you are ready to submit the full-scale workflow, you can do this in a non-interactive SLURM job using a batch script like this one:</p> <pre><code>#!/usr/bin/bash -l\n#SBATCH --job-name=&lt;snakemake_template&gt;\n#SBATCH --output=job_%j_%x.out\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1-00:00:00\n#SBATCH --mem=1G\n#SBATCH --mail-type=END,FAIL,TIME_LIMIT_90\n#SBATCH --mail-user=abc@bio.aau.dk\n\n# Exit on first error and if any variables are unset\nset -eu\n\n# Activate conda environment with only snakemake\nmamba activate &lt;snakemake_template&gt;\n\n# Start workflow using resources defined in the profile. Snakemake itself \n# requires nothing, 1CPU + 1G mem is enough\n\n# Render a DAG to visualize the workflow (optional)\nsnakemake --dag | dot -Tsvg &gt; results/dag.svg\n\n# Main workflow\nsnakemake --profile biocloud\n\n# Generate a report once finished (optional)\nsnakemake --report results/report.html\n</code></pre> <p>From this job Snakemake will submit individual SLURM jobs on your behalf for each task with the resources defined for each rule. This can sometimes start hundreds of jobs (of course within your limits) depending on the workflow and data - so please ensure you have defined a reasonable amount of resources for each rule and that the individual tasks utilize them as close to 100% as possible, especially CPUs. Often resource requirements depend on the exact input data or database being used, so it's also possible to dynamically scale the requirements using simple python lambda functions to calculate appropriate values for each resource, see Snakemake docs for details.</p>"},{"location":"guides/snakemake/biocloud/#biocloud-snakemake-profile","title":"BioCloud Snakemake profile","text":"<p>When using Snakemake to submit SLURM jobs the command with which to start the workflow can easily become quite long (as many as 20+ arguments). Using Snakemake profiles instead is an easy way to avoid this, where any command line options are simply written to a <code>config.yaml</code> file instead, which is then read by Snakemake using the <code>--profile</code> argument. On the BioCloud login and compute nodes there is already an optimized default profile pre-installed for all users in <code>/etc/xdg/snakemake/biocloud/config.yaml</code>, which will be automatically found by snakemake by name if you just type <code>snakemake --profile biocloud</code>. If you need to adjust this profile to suit your needs, you must copy it and place it in either the project folder from where snakemake will be run, or in the user standard location <code>~/.config/snakemake/biocloud/config.yaml</code> to avoid copying it around several times for each project. You must supply a name or a path to a folder in which a <code>config.yaml</code> is placed, not the path to the file itself. The default profile, which is optimized for our particular setup, is also shown below.</p> <pre><code>#command with which to submit tasks as SLURM jobs\ncluster:\n  mkdir -p logs/{rule}/ &amp;&amp;\n  sbatch\n    --parsable\n    --qos={resources.qos}\n    --cpus-per-task={threads}\n    --mem={resources.mem_mb}\n    --gpus={resources.gpus}\n    --time={resources.runtime}\n    --job-name=smk-{rule}-{wildcards}\n    --output=logs/{rule}/{rule}-{wildcards}-%j.out\n#if rules don't have resources set, use these default values.\n#Note that \"mem\" will be converted to \"mem_mb\" under the hood, so mem_mb is prefered\ndefault-resources:\n  - qos=\"normal\"\n  - threads=1\n  - mem_mb=512\n  - gpus=0\n  - runtime=\"0-01:00:00\"\n#max threads per job/rule. Will take precedence over anything else. Adjust this\n#before submitting to SLURM and leave threads settings elsewhere untouched\nmax-threads: 32\nuse-conda: False\nuse-singularity: False\nprintshellcmds: False\njobs: 50\nlocal-cores: 1\nlatency-wait: 120\nrestart-times: 0\nmax-jobs-per-second: 10 #don't touch\nkeep-going: True\nrerun-incomplete: True\nscheduler: greedy\nmax-status-checks-per-second: 5\ncluster-cancel: scancel\n#script to get job status for snakemake, unfortunately neccessary\ncluster-status: /etc/xdg/snakemake/biocloud/slurm-status.sh\n</code></pre> <p>Imagine writing all these settings on the command line every time - just no!</p> <p>The <code>slurm-status.sh</code> script on the last line is available from the template repository here or at <code>/etc/xdg/snakemake/biocloud/slurm-status.sh</code>.</p>"},{"location":"guides/snakemake/intro/","title":"Getting Started","text":""},{"location":"guides/snakemake/intro/#introduction","title":"Introduction","text":"<p>Snakemake is a workflow management system that simplifies the specification and execution of complex data processing and analysis pipelines using both self-made rules (tasks or steps) and/or reusable community-made wrappers. It's an open-source tool, designed to scale from simple to highly complex workflows, providing both flexibility and reproducibility for any computing environment. In comparison to shell scripts Snakemake is much more efficient because it automatically calculates rule dependencies between the individual rules depending on expected inputs and outputs as well as the input data itself, which greatly simplifies parallel computing. Snakemake workflows are written in Python in a standardized format, but has full support for any scripting language. It also has advanced features that automates the installation of required software tools ensuring complete portability and compatibility. Last, but not least, Snakemake has inherent support for cluster execution where workflow steps are translated and submitted as separate batch jobs with individually specified resource requirements to maximize resource utilization, while also managing the submission process, job completion monitoring, and much more.</p> <p>Snakemake provides detailed (and quite messy) documentation at snakemake.readthedocs.io, however the purpose of this guide is to boil things down a bit to help you get started on the BioCloud HPC for common use-cases and provide a standardized starting point for everyone. Let's get started.</p>"},{"location":"guides/snakemake/intro/#preparing-project-folder-for-snakemake","title":"Preparing project folder for Snakemake","text":"<p>The easiest way to get started with a new project is to create a new git repository on GitHub from our template GitHub repository. You of course need a GitHub account first, but that will also come in handy in many other situations. The template repository includes a minimal example workflow which will be explained on the next page, so that you can follow along. Clone the repo afterwards to your home folder somewhere and start filling in after this guide. According to the authors of Snakemake, the ideal way to structure a Snakemake project folder is like this (with a few additions):</p> <pre><code>project/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 analysis/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 config.yaml\n\u251c\u2500\u2500 data/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 profiles/\n\u251c\u2500\u2500 results/\n\u2514\u2500\u2500 workflow/\n    \u251c\u2500\u2500 envs/\n    \u2502   \u251c\u2500\u2500 tool1.yml\n    \u2502   \u2514\u2500\u2500 tool2.yml\n    \u251c\u2500\u2500 notebooks/\n    \u2502   \u251c\u2500\u2500 notebook1.ipynb\n    \u2502   \u2514\u2500\u2500 notebook2.ipynb\n    \u251c\u2500\u2500 report/\n    \u2502   \u251c\u2500\u2500 plot1.rst\n    \u2502   \u2514\u2500\u2500 plot2.rst\n    \u251c\u2500\u2500 rules/\n    \u2502   \u251c\u2500\u2500 rule1.smk\n    \u2502   \u2514\u2500\u2500 rule2.smk\n    \u251c\u2500\u2500 scripts/\n    \u2502   \u251c\u2500\u2500 script1.R\n    \u2502   \u251c\u2500\u2500 script2.sh\n    \u2502   \u2514\u2500\u2500 script3.py\n    \u2514\u2500\u2500 Snakefile\n</code></pre> <p>At first it might seem like a lot of files and folders, but workflows can grow quickly, so it's nice with a proper structure from the beginning. You can of course also put everything in a separate subfolder if developing a workflow is not the main goal of the project.</p>"},{"location":"guides/snakemake/intro/#installation","title":"Installation","text":"<p>To setup Snakemake use the <code>environment.yaml</code> file provided in the template repository to create a conda environment from a file for the project with <code>mamba env create -f environment.yml</code>.  You can also just create and activate a conda environment for the project on the command line: <pre><code>mamba create -c mamba-forge -c biomamba -n snakemake snakemake==7.18.2\nmamba activate snakemake\n</code></pre></p> Note: The Snakemake version matters <p>Snakemake keeps track of files but also the Snakemake version used to run the workflow. Therefore it is important to use the same version of snakemake when re-running a workflow. If you try to run a workflow with a different version of snakemake, it will re-run all samples already processed. Therefore be explicit about the version installed and ensure that it's written down somewhere, fx in the project <code>README</code> file or a conda <code>environment.yml</code> file.</p> <p>If you use Visual Studio Code, it can be handy to install the <code>snakemake</code> extension to enable syntax highlighting, linting, and autocompletion when developing workflows.</p>"},{"location":"guides/snakemake/intro/#workflow-catalogs","title":"Workflow catalogs","text":"<p>Before you go ahead and create a new workflow from scratch, it's a good idea to have a look in the public Snakemake workflow catalog, because someone might have already made something similar you could use instead. Also have a look at WorkflowHub for workflows made with Snakemake, but also other workflow tools.</p> <p>The template repository is also configured with a few standard GitHub Actions to automatically test the workflow, and passing some tests is required if you also wish to publish it to the catalog. As long as your repository is public and the <code>README.md</code> file contains the two words snakemake and workflow (and a few other rules for inclusion) it will show up in the catalogue automatically. You don't have to do anything specific for it to be listed there, they simply scan GitHub at regular intervals. This is the best way to share your workflow for others to use, but it's optional.</p> <p>Lastly, if you stick to the standardized folder structure above and keep the workflow in a GitHub repository (public or private) you can easily reuse your workflow across multiple projects using snakedeploy. This is also how workflows are reused from the catalog.</p>"},{"location":"guides/snakemake/tutorial/","title":"Workflow tutorial","text":"<p>To demonstrate how Snakemake is used in practice we'll go through an example workflow. To be able to follow along it's assumed that you have set up a project folder from the template git repository as described on the previous page.</p>"},{"location":"guides/snakemake/tutorial/#example-workflow","title":"Example workflow","text":"<p>The goal of this simple example workflow is to map some demultiplexed nanopore sequences to a database. This is done in two steps, which are called rules in Snakemake. The rules will do the following:</p> <ul> <li>Concatenate all <code>.fastq.gz</code> files in each subfolder in the input data folder into single files using the subfolder (barcode) names as sample IDs</li> <li>Map each file to a database using <code>minimap2</code> and <code>samtools</code> to produce a <code>.sam</code> file for each sample</li> </ul>"},{"location":"guides/snakemake/tutorial/#the-snakefile","title":"The <code>Snakefile</code>","text":"<p>The <code>workflow/Snakefile</code> is the main workflow file where all inputs and outputs are defined as well as the actual rules to run. To achieve our goal a <code>Snakefile</code> could look like this:</p> <p>workflow/Snakefile <pre><code>import os\nfrom snakemake.utils import min_version\n\n# minimum snakemake version required\nmin_version(\"7.18.2\")\n\n# config file path\nconfigfile: \"config/config.yaml\"\n\n# list all subfolders in the input_dir (defined in the config.yaml file)\nsample_dirs = os.listdir(config['input_dir'])\n\n# include rules\ninclude: \"rules/concatenate_fastq.smk\"\ninclude: \"rules/map2db.smk\"\n\n# define the expected output using wildcards\nrule all:\n  input:\n    expand(os.path.join(config['output_dir'], \"{sample}.sam\"), sample=sample_dirs)\n</code></pre> The <code>Snakefile</code> must always contain the special rule <code>all</code>. This rule is used to define a list with the expected output (confusingly listed under <code>input</code>) of the entire workflow, which can be dynamically generated depending on the input data using wildcards and the <code>expand()</code> function, which in this example will generate a list of output <code>.sam</code> files for each subfolder in the <code>input_dir</code> defined in the config file. Learning how to use wildcards is very important as it's required to enable many of the awesome features Snakemake provides, so it's highly recommended experimenting a bit with them and read the Snakemake docs.</p>"},{"location":"guides/snakemake/tutorial/#workflow-configuration-file","title":"Workflow configuration file","text":"<p>The <code>configfile</code> variable is a required Snakemake variable and must point to a config file, which in this example looks like this:</p> <p>config/config.yaml <pre><code>output_dir: \"results\"\ninput_dir: \"data/samples/\"\ntmp_dir: \"tmp\"\ndb_path: \"/databases/midas/MiDAS5.2_20231221/output/FLASVs.fa\"\nmax_threads: 128\n</code></pre></p> <p>The config file can contain anything, and is used to allow the user to customize how the workflow will run, which input and database files to use, individual settings for certain tools, and so on. Importantly, it should NOT contain any settings relevant for the exact computing setup and how things are run on a particular platform. For this Snakemake instead uses profiles that are supposed to be configured in <code>config.yaml</code> files under <code>profiles/</code> instead, more on them on the next page.</p>"},{"location":"guides/snakemake/tutorial/#rules","title":"Rules","text":"<p>Rules define the actual workflow steps and from these Snakemake will dynamically start one or more tasks depending on the input data and configuration. The two rules in the above example <code>Snakefile</code> are placed in two files separate from the <code>Snakefile</code> and imported using <code>include</code> statements to provide a better overview as the workflow grows, but they could also have been written directly in the <code>Snakefile</code> itself. The files can each contain any number of rules or arbitrary Python code. The two files for the workflow look like this:</p> <p>concatenate_fastq.smk <pre><code>import glob\nimport os\n\n# helper function to generate a list of all fastq.gz files \n# per wildcard (subfolder/sample).\n# See https://snakemake.readthedocs.io/en/latest/tutorial/advanced.html#step-3-input-functions\ndef listFastq(wildcards):\n  fastqs = glob.glob(os.path.join(config['input_dir'], wildcards.sample, \"*.fastq.gz\"))\n  return fastqs\n\nrule concatenate_fastq:\n  input:\n    listFastq\n  output:\n    temp(os.path.join(config['tmp_dir'], \"samples\", \"{sample}_concat.fastq.gz\"))\n  resources:\n    # calculate required memory based on input file size\n    # assuming it scales linearly, ensures a minimum of 512MB\n    mem_mb=lambda wc, input: max(3 * input.size_mb, 512)\n  threads: 1\n  log:\n    os.path.join(config[\"log_dir\"], \"concatenate_fastq\", \"{sample}.log\"),\n  shell:\n    \"cat {input} &gt; {output}\"\n</code></pre></p> <p>map2db.smk <pre><code>rule map2db:\n  input:\n    os.path.join(config['tmp_dir'], \"samples\", \"{sample}_concat.fastq.gz\")\n  output:\n    os.path.join(config['output_dir'], \"{sample}.sam\")\n  resources:\n    # calculate required memory based on input file size\n    # assuming it scales linearly, ensures a minimum of 10GB\n    mem_mb=lambda wc, input: max(3 * input.size_mb, 10240)\n  threads: config['max_threads']\n  params:\n    db_path = config['db_path']\n  conda:\n    \"../envs/map2db.yml\"\n  log:\n    os.path.join(config[\"log_dir\"], \"map2db\", \"{sample}.log\")\n  shell:\n    \"\"\"\n    minimap2 \\\n      -ax map-ont \\\n      -K20M \\\n      -t {threads} \\\n      --secondary=no \\\n      {params.db_path} \\\n      {input} \\\n      | samtools \\\n        view \\\n        -F 4 \\\n        -F 256 \\\n        -F 2048 \\\n        --threads $(nproc) \\\n        -o {output}\n    \"\"\"\n</code></pre></p> <p>Rules can also run any arbitrary scripts placed in the <code>workflow/scripts</code> folder instead of shell commands, which can be handy for more customization and longer commands, or to run R scripts and markdown files to produce some figures, perform analyses, and so on. Refer to the Snakemake docs for details about how to pass on variables from the Snakemake workflow to the scripts.</p>"},{"location":"guides/snakemake/tutorial/#example-output","title":"Example output","text":"<p>For now, it's more important that you understand the essential components of a workflow and what the resulting output from the above example looks like before learning how to properly run a Snakemake workflow on the BioCloud HPC. This will be described on the next page instead.</p> <p>When running the workflow above the <code>concatenate_fastq</code> rule will with the following example input data:</p> <pre><code>data/samples\n\u251c\u2500\u2500 barcode01\n\u2502   \u251c\u2500\u2500 FAW32656_pass_barcode01_06cd0bbc_287fe392_0.fastq.gz\n\u2502   \u2514\u2500\u2500 FAW32656_pass_barcode01_06cd0bbc_287fe392_1.fastq.gz\n\u251c\u2500\u2500 barcode02\n\u2502   \u251c\u2500\u2500 FAW32656_pass_barcode02_06cd0bbc_287fe392_0.fastq.gz\n\u2502   \u2514\u2500\u2500 FAW32656_pass_barcode02_06cd0bbc_287fe392_1.fastq.gz\n\u2514\u2500\u2500 barcode03\n    \u251c\u2500\u2500 FAW32656_pass_barcode03_06cd0bbc_287fe392_0.fastq.gz\n    \u2514\u2500\u2500 FAW32656_pass_barcode03_06cd0bbc_287fe392_1.fastq.gz\n</code></pre> <p>start 3 separate tasks that result in the following (temporary) output files: <pre><code>tmp\n\u2514\u2500\u2500 samples\n    \u251c\u2500\u2500 barcode01_concat.fastq.gz\n    \u251c\u2500\u2500 barcode02_concat.fastq.gz\n    \u2514\u2500\u2500 barcode03_concat.fastq.gz\n</code></pre></p> <p>As soon as any of the files have been produced by the 3 separate and totally independent <code>concatenate_fastq</code> tasks, the <code>map2db</code> tasks will start immediately regardless of whether all files have been produced first - no time lost. From the 3 temporary files the <code>map2db</code> rule will spawn 3 new separate tasks and produce the following files:</p> <pre><code>results\n\u251c\u2500\u2500 barcode01.sam\n\u251c\u2500\u2500 barcode02.sam\n\u2514\u2500\u2500 barcode03.sam\n</code></pre> <p>This is also the final output defined in the <code>all</code> rule, so once these 3 files have been created, Snakemake exits and we're done.</p> <p>If we now run the workflow again, Snakemake won't do anything unless specifically asked to, because the expected final output has already been produced previously. The same is true for the individual tasks and this is how Snakemake adds checkpoints to workflows. It will not start from scratch every time you run a workflow, but instead move on from the last checkpoint until the final output has been produced, which is very handy if (when) something fails.</p>"},{"location":"guides/snakemake/tutorial/#the-directed-acyclic-graph-dag","title":"The Directed Acyclic Graph (DAG)","text":"<p>The <code>Snakefile</code> will be read sequentially by Snakemake, however the exact order of the individual rules doesn't matter, since Snakemake will, before running anything, first build a dependency graph called the DAG (Directed Acyclic Graph) between all the rules depending on the <code>input</code>'s and <code>output</code>'s defined for each rule and the exact input data used. Some rules can therefore be run completely independently of eachother in parallel if they don't depend on eachother like in the example workflow above, while other rules will not run before an expected output from another rule has been produced first. This is a very powerful feature that is well-suited for HPC systems that use a job scheduling system like SLURM, because the tasks can be submitted by Snakemake as separate jobs with separate resource allocations and run simultaneously, while still under Snakemake's control. The workflow DAG can be visualized using the following command, and will for this particular example data with 3 samples look like this:</p> <pre><code>snakemake --dag | dot -Tsvg &gt; dag.svg\n</code></pre> <p></p> <p>If we now need to scale this up to a full nanopore flowcell with 96 barcodes, for example, Snakemake would simply submit 96 jobs at once instead of 3, while the time it takes to finish would be the same (of course as long as there are resources available). Awesome.</p>"},{"location":"guides/snakemake/tutorial/#software-deployment-methods","title":"Software deployment methods","text":"<p>To ensure the required software is available for each rule Snakemake supports both Conda environments and apptainer/singularity containers. You can add support for both at the same time by defining both <code>conda:</code> and <code>container:</code> at each rule definition, and then let the user decide which one to use.</p>"},{"location":"guides/snakemake/tutorial/#conda","title":"Conda","text":"<p>To use Conda environments you simply note the requirements down in a file, preferably with the exact versions used during development, and provide the path to <code>conda</code> at the rule definition like in the <code>map2db</code> rule above. An environment file might look like this:</p> <p>envs/map2db.yml <pre><code>name: map2db\nchannels:\n  - bioconda\ndependencies:\n  - minimap2=2.26\n  - samtools=1.18\n</code></pre></p> <p>Snakemake will then create the environment(s) the first time the workflow is run and automatically load the environment before running each task.</p>"},{"location":"guides/snakemake/tutorial/#containers","title":"Containers","text":"<p>To use apptainer/singularity containers you just supply the name of the container in a container registry, for example <code>container: docker://biocontainers/minimap2</code>. If you need a container with multiple tools at once see multi-package containers.</p>"},{"location":"guides/snakemake/tutorial/#snakemake-wrappers","title":"Snakemake wrappers","text":"<p>The above example is rather simple and is only made for demonstrating the basics of Snakemake workflows. But if this was part of a real workflow, however, it would not be the proper way to achieve our particular goals, because someone has of course already used minimap2 in a Snakemake workflow before. So we could have saved some time and effort and instead used one of the many community-made Snakemake wrappers for each rule. The format is exactly the same as a normal rule where you define <code>input</code>'s and <code>output</code>'s etc, but the exact command(s) that the rule would normally run is replaced with fx <code>wrapper: \"v3.3.4/bio/minimap2/aligner\"</code>. If rules for a certain tool isn't available already - you can contribute too!</p>"},{"location":"guides/webportal/files/","title":"File browser","text":"<p>Browsing around directories in the terminal can be quite inconvenient, especially when you need to move or transfer many files. Using the file browser in the web portal is much simpler. To browse and transfer files click Files at the menu bar and then the desired starting folder, for example your home directory:</p> <p></p> <p>Everything here is pretty self-explanatory. Click Upload to upload a file, Download to download a file, select multiple files and/or folders at once for bulk actions, etc. Here's a screenshot of how it looks:</p> Avoid using the file browser for large data transfers <p>For larger data transfers it's best to do it over the SSH protocol using tools like <code>scp</code> or <code>sftp</code> through programs like WinSCP, FileZilla, or just a terminal.</p> <p></p>"},{"location":"guides/webportal/jobcomposer/","title":"Job composer","text":"<p>Guide coming soon...</p>"},{"location":"guides/webportal/jobqueue/","title":"Job queue","text":"<p>You can browse the SLURM job queue and see details of all queued and running jobs by clicking Jobs and then Active jobs:</p> <p></p> <p>By default you will only see your own jobs, so it might be empty at first. To see the entire queue and the running jobs from all users click Your Jobs at the top right and then select All Jobs:</p> <p></p> <p>You can now see a list of all queued and running jobs as well as recently completed jobs for each compute node partition (confusingly named Queue):</p> <p></p> <p>To see additional details of individual jobs click the little arrow to the left of each job:</p> <p></p>"},{"location":"guides/webportal/shell/","title":"Shell access","text":"<p>Obtain a shell in the browser by clicking Clusters and then biocloud Shell Access:</p> <p></p> <p>Enter your password when prompted:</p> <p></p> <p>And you should now see the message of the day and a prompt in the exact same way as when accessing through SSH:</p> <p></p> <p>You can even choose between different themes at the top right if you want.</p>"},{"location":"guides/webportal/apps/coder/","title":"Code server","text":"<p>Code Server is an open source alternative to Visual Studio Code provided by Coder, which can be run through a web browser. It's based on much of the same code base and has the exact same features overall, however there are minor differences. If you want the full VS Code experience you must follow this guide instead. This app will allow you to run a Code Server in a SLURM job and access it directly from your browser.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/coder/#starting-the-app","title":"Starting the app","text":"<p>Start by selecting the desired working directory where Code will start (you can also change this inside), and the amount of resources that you expect to use and for how long:</p> <p></p> <p>If you need to use a node with specific features, for example if you need some fast and local scratch space, or otherwise need to pass any additional options to the Slurm <code>sbatch</code> command used to launch the job, you can enter them in the \"additional job options\" field. Then click Launch!</p>"},{"location":"guides/webportal/apps/coder/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen compute node partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the job has been granted a resource allocation the server needs a little while to start and you will see the status change to Starting</p> <p></p> <p>and when it finishes a button will appear to launch Code Server:</p> <p></p> <p>You can now start working:</p> <p></p> <p>Hit ctrl+j to launch a terminal</p> Closing the window <p>If you close the window or browser tab while something is running in the terminal, it will NOT continue to run in the background.</p>"},{"location":"guides/webportal/apps/coder/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. You can do that by clicking the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used, and to reduce queue time.</p>"},{"location":"guides/webportal/apps/jupyter/","title":"Jupyter notebook","text":"<p>Jupyter Notebook is a web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience. This app will allow you to run a Jupyter Notebook server in a SLURM job and access it directly from your browser.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/jupyter/#starting-the-app","title":"Starting the app","text":"<p>Start by selecting the amount of resources that you expect to use and for how long:</p> <p></p> <p>If you need to use a node with specific features, for example if you need some fast and local scratch space, or otherwise need to pass any additional options to the Slurm <code>sbatch</code> command used to launch the job, you can enter them in the \"additional job options\" field. Then click Launch!</p>"},{"location":"guides/webportal/apps/jupyter/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen compute node partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the job has been granted a resource allocation the server needs a little while to start and you will see the status change to Starting</p> <p></p> <p>and when it finishes a button will appear to launch the notebook:</p> <p></p> <p>You can now start working:</p> <p></p> Closing the window <p>If you close the window or browser tab while something is running, it will continue to run in the background. You can always reconnect to it by clicking the Connect to Jupyter Notebook button again.</p>"},{"location":"guides/webportal/apps/jupyter/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. You can do that by either clicking the Quit button inside the notebook server in the top right corner:</p> <p></p> <p>or click the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used, and to reduce queue time.</p>"},{"location":"guides/webportal/apps/jupyter/#installing-python-packages-for-jupyter","title":"Installing Python packages for Jupyter","text":""},{"location":"guides/webportal/apps/jupyter/#from-pypi","title":"From PyPI","text":"<p>If the exact Python version isn't important to you, you can install packages from pipy using <code>pip</code> directly from a notebook using for example <code>!pip install matplotlib</code>. However, this will use the default python version installed on the system, which cannot be changed and will be upgraded from time to time, which can result in incompatibilities and break dependencies between packages after each upgrade. To manage both the python version and python packages it's better to use conda environments.</p>"},{"location":"guides/webportal/apps/jupyter/#using-conda-environments","title":"Using conda environments","text":"<p>You can use conda environments to manage software and python packages and make them available for Jupyter notebooks by installing <code>ipykernel</code> into any environment as a separate kernel:</p> <pre><code>mamba activate myproject\nmamba install ipykernel\npython -m ipykernel install --user --name myproject --display-name myproject\n</code></pre> <p>This will install the kernel into <code>${HOME}/.local/share/jupyter/kernels</code> with the chosen display name and make it visible from Jupyter. Select the new kernel either when creating a new notebook:</p> <p></p> <p>or when already working on a notebook by changing kernel from the menus:</p> <p></p> <p>In a similar way you can also run R code in Jupyter notebooks by installing the IRkernel into an environment: <pre><code>mamba activate myproject\nmamba install r-irkernel\nR -e \"IRkernel::installspec(name = 'myproject-Rkernel', displayname = 'myproject-Rkernel')\"\n</code></pre></p> <p>You should see the R logo when you've chosen the correct kernel:</p> <p></p> <p>If you need to install R packages, don't run any <code>install.packages()</code> commands, you must install R packages into the conda environment instead.</p>"},{"location":"guides/webportal/apps/rstudio/","title":"RStudio","text":"<p>RStudio is an integrated development environment (IDE) for R and Python. It includes a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging, and workspace management. This app will allow you to run an RStudio server in a SLURM job and access it directly from your browser.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/rstudio/#starting-the-app","title":"Starting the app","text":"<p>Start by selecting the desired R version and the amount of resources that you expect to use and for how long:</p> <p></p> <p>If you need to use a node with specific features, for example if you need some fast and local scratch space, or otherwise need to pass any additional options to the Slurm <code>sbatch</code> command used to launch the job, you can enter them in the \"additional job options\" field. Then click Launch!</p>"},{"location":"guides/webportal/apps/rstudio/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen compute node partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the job has been granted a resource allocation the server needs a little while to start and you will see the status change to Starting</p> <p></p> <p>and when it finishes a button will appear to launch RStudio:</p> <p></p> <p>You can now start working:</p> <p></p> Closing the window <p>If you close the window or browser tab while something is running, it will continue to run in the background. You can always reconnect to it by clicking the Connect to RStudio Server button again.</p>"},{"location":"guides/webportal/apps/rstudio/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. Before you do that, however, it's a good idea to first click the little red button inside RStudio in the top right corner to shut down the R session gracefully:</p> <p></p> <p>Then stop the job by clicking the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used, and to reduce queue time.</p>"},{"location":"guides/webportal/apps/rstudio/#containerization","title":"Containerization","text":"<p>The RStudio server runs in the SLURM job from within a singularity/apptainer container, that is based on Rocker container images. This means that R packages installed from the RStudio app may not work with other R installations due to different base operating system packages, so the RStudio app uses a different R library location by default, which is located under <code>$HOME/R/rstudio-server/R_MAJOR_VERSION</code>.</p> <p>Furthermore, because RStudio is running in an isolated Linux container, you cannot for example load your usual conda environments or issue SLURM commands from the integrated terminal, etc. Only the Ceph network storage mount points are available at their usual locations, but everything else is otherwise completely isolated from the host on which the container runs.</p>"},{"location":"guides/webportal/apps/rstudio/#known-issues","title":"Known issues","text":"<p>If you only see a gray background when starting the RStudio Server, you may have a stuck R session if it was abruptly terminated due to job time limit or it ran out of memory etc. To resolve this problem delete the <code>~/.local/share/rstudio</code> folder to reset everything. This will also delete any unsaved files you've had open recently, so if you want you can also just rename the folder instead to fx <code>rstudio_backup</code> to create a backup.</p>"},{"location":"guides/webportal/apps/virtualdesktop/","title":"Virtual Desktop","text":"<p>You can start a virtual desktop on any compute node and access it directly through the browser without having to install and set up any software on your own computer first to access it. This is useful for running GUI software that doesn't run from a command line, but instead needs to show up in graphical windows where you can click around like you are used to on your personal computer.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/virtualdesktop/#starting-the-app","title":"Starting the app","text":"<p>Click the Virtual Desktop app icon on the front page or select it from the Interactive Apps menu bar at the top:</p> <p></p> <p>Start by selecting the amount of resources that you expect to use and for how long:</p> <p></p> <p>If you need to use a node with specific features, for example if you need some fast and local scratch space, or otherwise need to pass any additional options to the Slurm <code>sbatch</code> command used to launch the job, you can enter them in the \"additional job options\" field. Then click Launch!</p>"},{"location":"guides/webportal/apps/virtualdesktop/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen compute node partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the SLURM scheduler has granted an allocation for the job, you will see the status change to Running and a button will appear to launch the virtual desktop:</p> <p></p> <p>Ensure that the Image Quality slider is maximized. You don't need to adjust the Compression level unless you are on a poor network connection, in which case you can increase it. You can also share the session with other people, which can be handy for fx teaching. Now, click Launch, and you should see a desktop like this within a few seconds:</p> <p></p> <p>You can now start running anything you want, browse the Applications menu at the top left to find and launch your software.</p> Closing the window <p>If you close the window or browser tab while something is running, it will continue to run in the background. You can always reconnect to it by clicking the Launch Virtual Desktop button again.</p>"},{"location":"guides/webportal/apps/virtualdesktop/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. You can do that by either clicking Log Out inside the virtual desktop in the top right corner:</p> <p></p> <p>or click the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used, and to reduce queue time.</p>"},{"location":"slurm/accounting/","title":"Usage accounting and priority","text":"<p>All users belong to an account (usually their PI) where all usage is tracked on a per-user basis, which directly impact the priority of future jobs. Additionally, limitations and extra high priorities can be obtained by submitting jobs to different SLURM \"Quality of Service\"s (QOS). By default, all users are only allowed to submit jobs to the <code>normal</code> QOS with equal resource limits and base priority for everyone. Periodically users may submit to the <code>fastq</code> QOS instead, which has higher resource usage limits and a base priority higher than everyone else (and therefore the usage is also billed 3x), however this must first be discussed among the owners of the hardware (PI's), and then you can contact an administrator to grant your user permission to submit jobs to it for a limited period of time.</p>"},{"location":"slurm/accounting/#job-scheduling","title":"Job scheduling","text":"<p>BioCloud uses the first-in-first-out (FIFO) scheduling algorithm with backfilling enabled, which means that occasionally lower priority jobs may start before higher priority jobs to maximize the utilization of the cluster, since smaller (usually shorter) jobs may be able to finish before the larger jobs are scheduled to start. To increase the chance of backfilling, it is therefore important to set a realistic timelimit for your jobs that is as short as possible, but long enough for it to finish.</p> <p></p> Plan ahead - Make reservations! <p>To avoid any queue time altogether, resources can be reserved in advance for a user or account to ensure that jobs will start immediately when submitted, even if the partition or cluster is fully utilized. Data processing- and analysis time should already be an integral part of your project planning, so you might as well make a corresponding SLURM reservation too to avoid delays. Utilizing reservations can also help distribute the cluster workload over time more evenly, which is beneficial for everyone. There are several different types of reservations. They can be made by contacting an administrator.</p>"},{"location":"slurm/accounting/#job-priority","title":"Job priority","text":"<p>When a job is submitted a priority value between 1-1000 is calculated based on several factors, where a higher number indicates a higher priority in the queue. This does not impact running jobs, and the effect of prioritization is only noticable when the cluster is operating near peak capacity, or when the compute node partition to which the job has been submitted is fully allocated by running jobs. Otherwise jobs will usually start immediately as long as there are resources available and you haven't reached any usage limits.</p> <p>Different weights are given to the individual priority factors, where the most significant ones are the account fair-share factor and the QOS. All factors are normalized to a value between 0-1, then weighted by an adjustable scalar, which may be adjusted be occasionally by an administrator depending on the overall cluster usage over time. Users can also be nice to other users and reduce the priority of their own jobs by setting a \"nice\" value using <code>--nice</code> when submitting for example less time-critical jobs. Job priorities are then calculated according to the following formula:</p> <pre><code>Job_priority =\n    (PriorityWeightAge) * (age_factor) +\n    (PriorityWeightFairshare) * (fair-share_factor) +\n    (PriorityWeightJobSize) * (job_size_factor) -\n    - nice_factor\n</code></pre> <p>To see the currently configured priority factor weights use <code>sprio -w</code>: <pre><code>$ sprio -w\n  JOBID PARTITION   PRIORITY       SITE        AGE  FAIRSHARE    JOBSIZE\nWeights                               1        200        700       1920\n</code></pre></p> <p>The priority of pending jobs will be shown in the job queue when running <code>squeue</code>. To see the exact contributions of each factor to the priority of a pending job use <code>sprio -j &lt;jobid&gt;</code>: <pre><code>$ sprio -j 2282256\n  JOBID PARTITION   PRIORITY       SITE        AGE  FAIRSHARE    JOBSIZE\n2282256 zen3,zen5        586          0        129        619        138\n</code></pre></p> <p>The job age and size factors are important to avoid the situation where large jobs can get stuck in the queue for a long time because smaller jobs will always fit in everywhere much more easily. The age factor will max out to <code>1.0</code> when 3 days of queue time has been accrued for any job. The job size factor is directly proportional to the number of CPUs requested, regardless of the time limit, and is normalized to the total number of CPUs in the cluster. Therefore <code>PriorityWeightJobSize</code> is configured to be equal to the total number of (physical) CPU cores available in the cluster.</p>"},{"location":"slurm/accounting/#the-fair-share-factor","title":"The fair-share factor","text":"<p>As the name implies, the fair-share factor is used to ensure that users within each account have their fair share of computing resources made available to them over time. Because the individual research groups have contributed with different amounts of hardware to the cluster, the overall share of computing resources made available to them should match accordingly. Secondly, the resource usage of individual users within each account is important to consider as well, so that users who may recently have vastly overused their shares within each account will not have the highest priority. The goal of the fair-share factor is to balance the usage of all users by adjusting job priorities, so that it's possible for everyone to use their fair share of computing resources over time. The fair-share factor is calculated according to the fair-tree algorithm, which is an integrated part of the SLURM scheduler. At the time of writing, it has been configured with a usage decay half-life of 2 weeks, and the usage data is never reset.</p> <p>To see the current fair-share factor for your user and the amount of shares available for each account etc, you can run <code>sshare</code>:</p> <pre><code>$ sshare\nAccount                    User  RawShares  NormShares    RawUsage  EffectvUsage  FairShare\n-------------------- ---------- ---------- ----------- ----------- ------------- ----------\nroot                                          0.000000  2837264594      1.000000\n ao                                  20000    0.006428      304293      0.000107\n cms                                330985    0.106381    12136550      0.004278\n cv                                  20000    0.006428        3557      0.000001\n jln                                464042    0.149146   402995207      0.142037\n kln                                 20000    0.006428     4674172      0.001647\n kt                                 235183    0.075590   339372940      0.119613\n ma                                 602384    0.193611  1134841454      0.399977\n md                                 321172    0.103227   510724111      0.180006\n ml                                  20000    0.006428     8882888      0.003131\n mms                                235183    0.075590    83145730      0.029304\n mrr                                145084    0.046631       26658      0.000009\n mto                                 20000    0.006428           0      0.000000\n ndj                                 20000    0.006428      493385      0.000174\n pc                                 290168    0.093262   274092835      0.096605\n phn                                214114    0.068818    54880782      0.019343\n  phn                abc@bio.a+          1    0.058824      903589      0.016465   0.298701\n pk                                  20000    0.006428          48      0.000000\n rw                                  20000    0.006428           0      0.000000\n sb                                  20000    0.006428         207      0.000000\n sss                                 33000    0.010606     7714721      0.002719\n students                            20000    0.006428     2917466      0.001028\n tnk                                 20000    0.006428           0      0.000000\n ts                                  20000    0.006428        2633      0.000001     \n</code></pre> <ul> <li><code>RawShares</code>: the amount of \"shares\" assigned to each account (in our setup simply the number of CPUs each account has contributed with)</li> <li><code>NormShares</code>: the fraction of shares given to each account normalized to the total shares available across all accounts, e.g. a value of 0.33 means an account has been assigned 33% of all the resources available in the cluster.</li> <li><code>RawUsage</code>: usage of all jobs charged to the account or user. The value will decay over time depending on the usage decay half-life configured. The <code>RawUsage</code> for an account is the sum of the <code>RawUsage</code> for each user within the account, thus indicative of which users have contributed the most to the account\u2019s overall score.</li> <li><code>EffectvUsage</code>: <code>RawUsage</code> divided by the total <code>RawUsage</code> for the cluster, hence the column always sums to <code>1.0</code>. <code>EffectvUsage</code> is therefore the percentage of the total cluster usage the account has actually used (in relation to the total usage, NOT the total capacity). In the example above, the <code>ma</code> account has used <code>26.5%</code> of the total cluster usage since the last usage reset, if any.</li> <li><code>FairShare</code>: The fair-share score calculated using the following formula <code>FS = 2^(-EffectvUsage/NormShares)</code>. The <code>FairShare</code> score can be interpreted by the following intervals: <ul> <li>1.0: Unused. The account has not run any jobs since the last usage reset, if any.</li> <li>0.5 - 1.0: Underutilization. The account is underutilizing their granted share. For example a value of 0.75 means the account has underutilized their share 1:2</li> <li>0.5: Average utilization. The account on average is using exactly as much as their granted share.</li> <li>0.0 - 0.5: Over-utilization. The account is overusing their granted share. For example a value of 0.75 means the account has recently overutilized their share 2:1</li> <li>0: No share left. The account is vastly overusing their granted share and users will get the lowest possible priority.</li> </ul> </li> </ul> <p>For more details about job prioritization see the SLURM documentation and this presentation.</p>"},{"location":"slurm/accounting/#usage-limits-and-qos","title":"Usage limits and QOS","text":"<p>Usage limits are set at the QOS level, where the QOS's listed below are present in the cluster. The <code>normal</code> QOS is the default, and the <code>fastq</code> QOS is only available to users who have been granted permission by an administrator for a limited period of time to get a higher priority than everyone else and higher usage limits. The <code>interactive</code> QOS is only available on the <code>interactive</code> partition and will be enforced on all jobs submitted to the <code>interactive</code> partition.</p> <p>Please note that the following limits may occasionally be adjusted without further notice to balance usage, so this table may not always be completely up to date. You can always use the <code>sacctmgr show qos</code> command to see the current limits for each QOS.</p> <code>interactive</code> <code>normal</code> <code>fastq</code> UsageFactor Usage accounting will be multiplied by this value 1.0 1.0 3.0 Priority Add this number to the calculated job priority +1000 MinPrioThres Minimum priority required for the job to be scheduled MaxTRESPU Maximum number of TRES (trackable resources) each user is able to use at once cpu=64,mem=256G cpu=864 MaxJobsPU Maximum number of jobs each user is able to have running at once 10 200 500 MaxSubmitPU Maximum number of jobs each user is able to submit at once (running+pending) 20 500 1000 MaxTRESPA Maximum number of TRES (trackable resources) each account is able to use at once cpu=1760 cpu=1760 MaxTRESRunMinsPA Maximum number of TRES*minutes each account is able to have running at once (e.g. 100CPU's for 1 hour corresponds to 6000 CPU minutes) cpu=18000000 cpu=30000000 MaxTRESRunMinsPU Maximum number of TRES*minutes each user is able to have running at once (e.g. 100CPU's for 1 hour corresponds to 6000 CPU minutes) cpu=8000000 <p>To see details about account associations, allowed QOS's, limits set at the user level, and more, for your user, use the following command: <pre><code># your user\n$ sacctmgr show user withassoc where name=$USER\n      User   Def Acct     Admin    Cluster    Account  Partition     Share   Priority MaxJobs MaxNodes  MaxCPUs MaxSubmit     MaxWall  MaxCPUMins                  QOS   Def QOS\n---------- ---------- --------- ---------- ---------- ---------- --------- ---------- ------- -------- -------- --------- ----------- ----------- -------------------- ---------\nabc@bio.a+       root      None   biocloud       root                    1          1                                                                  fastq,normal    normal\n\n# all users\n$ sacctmgr show user withassoc | less\n</code></pre></p>"},{"location":"slurm/intro/","title":"Introduction to SLURM","text":"<p>SLURM (Simple Linux Utility for Resource Management) is a highly flexible and powerful job scheduler for managing and scheduling computational workloads on high-performance computing (HPC) clusters. SLURM is designed to efficiently allocate resources and manage job execution on clusters of any size, from a single server to tens of thousands. SLURM manages resources on an HPC cluster by dividing similar compute nodes into partitions. Users submit jobs with specified resource requirements to these partitions from a login-node, and then the SLURM controller schedules and allocates resources to those jobs based on available resources. SLURM also stores detailed usage information of all jobs in a usage accounting database, which allows enforcement of fair-share policies and priorities for job scheduling for each partition.</p>"},{"location":"slurm/intro/#biocloud-slurm-cluster-overview","title":"BioCloud SLURM cluster overview","text":"<p>(Note: the exact partitions in the figure may be outdated, but the setup is the same)</p>"},{"location":"slurm/intro/#getting-started","title":"Getting started","text":"<p>Start with obtaining shell access to one of the login nodes <code>bio-fe[01-02].srv.aau.dk</code>, as described on the SSH access page. To start with it's always nice to get an overview of the cluster, it's partitions, and how many resources that are currently allocated. This is achieved with the <code>sinfo</code> command, example output:</p> <pre><code>$ sinfo\n  PARTITION AVAIL  TIMELIMIT  CPUS(A/I/O/T)     STATE       REASON  NODES        NODELIST AVAIL_FEATURES\ninteractive    up 1-00:00:00   70/218/0/288     mixed         none      1      bio-node11 zen5,epyc9565\n       zen5    up 14-00:00:0  456/120/0/576     mixed         none      2 bio-node[12-13] zen5,epyc9565\n       zen5    up 14-00:00:0  106/150/0/256     mixed         none      1      bio-node17 zen5,epyc9535\n       zen5    up 14-00:00:0    0/256/0/256      idle         none      1      bio-node16 zen5,epyc9535\n      zen3*    up 14-00:00:0   164/92/0/256     mixed         none      1      bio-node01 scratch,zen3,epyc7713\n      zen3*    up 14-00:00:0   46/146/0/192     mixed         none      1      bio-node02 zen3,epyc7552\n      zen3*    up 14-00:00:0  270/306/0/576     mixed         none      3 bio-node[03,06- zen3,epyc7643\n      zen3*    up 14-00:00:0   90/102/0/192     mixed         none      1      bio-node05 scratch,zen3,epyc7643\n      zen3*    up 14-00:00:0    192/0/0/192 allocated         none      1      bio-node04 zen3,epyc7643\n      zen5x    up 14-00:00:0  276/300/0/576     mixed         none      2 bio-node[14-15] scratch,zen5,epyc9565\n      zen3x    up 14-00:00:0   164/28/0/192     mixed         none      1      bio-node08 zen3,epyc7643\n      zen3x    up 14-00:00:0  100/156/0/256     mixed         none      1      bio-node09 zen3,epyc7713\n    gpu-a10    up 14-00:00:0      0/64/0/64      idle         none      1      bio-node10 scratch,zen3,epyc7313,a10\n</code></pre> <p>To get an overview of running jobs use <code>squeue</code>, example output: <pre><code># everything\n$ squeue\n  JOBID              NAME       USER  ACCOUNT        TIME   TIME_LEFT CPU MIN_MEM ST PRIO   PARTITION NODELIST(REASON)\n2144804 OOD-RStudioServer user01@stu students     4:13:59     4:46:01  10     64G  R  340 interactive bio-node11\n2144803 OOD-VirtualDeskto user02@bio      kln     5:07:40     6:52:20  12     24G  R  325 interactive bio-node11\n2144808 OOD-RStudioServer user03@bio      phn     3:14:15     1:45:45  20    100G  R  270 interactive bio-node11\n2144913    OOD-CodeServer user04@bio       md       46:11     7:13:49  10     20G  R  142 interactive bio-node11\n2144912 OOD-RStudioServer user04@bio       md       46:21     7:13:39   4     25G  R  141 interactive bio-node11\n2144807 OOD-RStudioServer user05@bio       ma     3:31:15     8:28:45   1     12G  R   78 interactive bio-node11\n2144806    OOD-CodeServer user05@bio       ma     3:31:28     8:28:32   1      8G  R   78 interactive bio-node11\n2144816 OOD-RStudioServer user06@bio       ma     2:51:17     5:08:43  10     50G  R   75 interactive bio-node11\n2141338       semibin.S16 user07@bio       md  3-00:49:43 10-07:10:17  64    300G  R  351        zen3 bio-node04\n2141351       concoct.S16 user07@bio       md  2-23:31:34 10-08:28:26  32    300G  R  342        zen3 bio-node01\n2141299 817356b7-2ccc-4b9 user08@bio       ma  1-04:52:39  6-19:07:21  90    900G  R  284        zen3 bio-node03\n2141298 817356b7-2ccc-4b9 user08@bio       ma  1-05:47:55  6-18:12:05  90    900G  R  284        zen3 bio-node07\n2141297 817356b7-2ccc-4b9 user08@bio       ma  1-08:20:52  6-15:39:08  90    900G  R  284        zen3 bio-node05\n2141290 817356b7-2ccc-4b9 user08@bio       ma  1-13:22:11  6-10:37:49  90    900G  R  284        zen3 bio-node06\n2141259           mmlong2 user08@bio       ma  3-08:33:36  5-15:26:24  65    300G  R  278        zen3 bio-node01\n\n# your own jobs only\n$ squeue --me\n JOBID         NAME       USER       TIME    TIME_LEFT CPU MIN_ME ST PARTITION NODELIST(REASON)\n  3333 as-predictio user09@bio 2-19:42:49   6-04:17:11   5    16G  R   gpu-a10 bio-node10\n</code></pre></p> <p>Or get a more detailed overview per compute node of current resource allocations and which jobs are running etc. This will normally show some colored bars, but they are not visible here. <pre><code>$ sstatus\nCluster allocation summary per partition or individual nodes (-n).\n(Numbers are reported in free/allocated/total).\n\nPartition   |              CPUs               |           Memory (GB)            |       GPUs        |\n=====================================================================================================\ninteractive |  220 68                  /288   | 1198 303                 /1501   |           \nzen5        |  537 551                 /1088  | 2572 3433                /6005   |           \nzen3        |  649 759                 /1408  |  967 5512                /6479   |           \nzen5x       |  300 276                 /576   |  942 3572                /4514   |           \nzen3x       |  184 264                 /448   |  604 3383                /3987   |           \ngpu-a10     |   64 0                   /64    |  241 0                   /241    |    2 0         /2    \n-----------------------------------------------------------------------------------------------------\nTotal:      | 1954 1918                /3872  | 6525 16204               /22729  |    2 0         /2    \n\nTotal resources requested from queued jobs:\n  CPUs: 74 (3.6K CPU hours)\n\nJobs running/queued/total:\n  42 / 1 / 43\n\nUse sinfo or squeue to obtain more details.\n</code></pre></p>"},{"location":"slurm/jobcontrol/","title":"Job control and useful commands","text":"<p>Below are some nice-to-know commands for controlling and checking up on running or queued jobs.</p>"},{"location":"slurm/jobcontrol/#overall-cluster-status","title":"Overall cluster status","text":"<p>This will normally show some colored bars for each partition, which unfortunately doesn't render here. <pre><code>$ sstatus -n\nCluster allocation summary per partition or individual nodes (-n).\n(Numbers are reported in free/allocated/total(OS factor)).\n\nNode       | Partition   |              CPUs               |           Memory (GB)            |      GPUs       |\n================================================================================================================\nbio-node11 | interactive |  220 68                  /288   | 1198 303                 /1501   |           \nbio-node12 | zen5        |  120 168                 /288   |   14 1487                /1501   |           \nbio-node13 | zen5        |    3 285                 /288   |  487 1014                /1501   |           \nbio-node16 | zen5        |  182 74                  /256   |  418 1083                /1501   |           \nbio-node17 | zen5        |  158 98                  /256   |  569 932                 /1501   |           \nbio-node01 | zen3        |   94 162                 /256   |   97 900                 /997    |           \nbio-node02 | zen3        |  147 45                  /192   |  293 200                 /493    |           \nbio-node03 | zen3        |  102 90                  /192   |   97 900                 /997    |           \nbio-node04 | zen3        |    0 192                 /192   |  185 812                 /997    |           \nbio-node05 | zen3        |  102 90                  /192   |   97 900                 /997    |           \nbio-node06 | zen3        |  102 90                  /192   |   97 900                 /997    |           \nbio-node07 | zen3        |  102 90                  /192   |   97 900                 /997    |           \nbio-node14 | zen5x       |  186 102                 /288   |  807 1450                /2257   |           \nbio-node15 | zen5x       |  114 174                 /288   |  135 2122                /2257   |           \nbio-node08 | zen3x       |   28 164                 /192   |   10 1983                /1993   |           \nbio-node09 | zen3x       |  156 100                 /256   |  593 1400                /1993   |           \nbio-node10 | gpu-a10     |   64 0                   /64    |  241 0                   /241    |    2 0         /2    \n-----------------------------------------------------------------------------------------------------------------\nTotal:                   | 1880 1992                /3872  | 5441 17288               /22729  |    2 0         /2    \n\nJobs running/queued/total:\n  43 / 0 / 43\n\nUse sinfo or squeue to obtain more details.\n</code></pre></p>"},{"location":"slurm/jobcontrol/#get-job-status-info","title":"Get job status info","text":"<p>Use <code>squeue</code>, for example: <pre><code>$ squeue\nsqueue\n      JOBID             NAME       USER ACCOUNT        TIME   TIME_LEFT CPU MIN_ME ST PRIO  PARTITION NODELIST(REASON)\n    1275175    RStudioServer user01@bio     acc1       0:00  3-00:00:00  32     5G PD    4       zen3 (QOSMaxCpuPerUserLimit)\n    1275180       sshdbridge user02@bio     acc2       7:14     7:52:46   8    40G  R    6       zen3 bio-node03\n    1275170   VirtualDesktop user03@bio     acc2      35:54     5:24:06   2    10G  R    6       zen3 bio-node05\n</code></pre></p> <p>To show only your own jobs use <code>squeue --me</code>. This is used quite often so <code>sq</code> has been made an alias of <code>squeue --me</code>. You can for example also append <code>--partition</code>, <code>--nodelist</code>, <code>--reservation</code>, and more, to only show the queue for those select partitions, nodes, or reservations.</p> <p>You can also get an estimated start time for pending jobs by using <code>squeue --start</code>. Jobs will in most cases start earlier than this time, as the calculation is based on the time limit set for current running jobs, and most finish in time. They can only start later if new jobs with a higher priority get submitted to the queue before they start:</p> <pre><code>$ squeue --start\n    JOBID PARTITION     NAME     USER ST          START_TIME  NODES SCHEDNODES           NODELIST(REASON)\n  1306529      zen3 smk-mapp user01@b PD 2025-01-24T12:25:15      1 bio-node02        (Resources)\n  1306530      zen3 smk-mapp user01@b PD 2025-01-24T12:25:15      1 bio-node04        (Priority)\n  1309386      zen3 cq_EV_me user02@b PD 2025-01-24T12:25:15      1 bio-node05        (Priority)\n  1306531      zen3 smk-mapp user01@b PD 2025-01-24T12:27:02      1 bio-node03        (Priority)\n  1306532      zen3 smk-mapp user01@b PD 2025-01-25T14:46:56      1 (null)               (Priority)\n  1306533      zen3 smk-mapp user01@b PD 2025-01-25T14:46:56      1 (null)               (Priority)\n...\n</code></pre> Job state codes (ST) Status  Code Explaination COMPLETED CD COMPLETING CG FAILED F PENDING PD PREEMPTED PR RUNNING R SUSPENDED S STOPPED ST <p>A complete list can be found in SLURM's documentation</p> Job reason codes (REASON ) Reason Code Explaination Priority One or more higher priority jobs is in queue for running. Your job will eventually run. Dependency This job is waiting for a dependent job to complete and will run afterwards. Resources The job is waiting for resources to become available and will eventually run. InvalidAccount The job\u2019s account is invalid. Cancel the job and rerun with correct account. InvaldQoS The job\u2019s QoS is invalid. Cancel the job and rerun with correct account. QOSGrpCpuLimit All CPUs assigned to your job\u2019s specified QoS are in use; job will run eventually. QOSGrpMaxJobsLimit Maximum number of jobs for your job\u2019s QoS have been met; job will run eventually. QOSGrpNodeLimit All nodes assigned to your job\u2019s specified QoS are in use; job will run eventually. PartitionCpuLimit All CPUs assigned to your job\u2019s specified partition are in use; job will run eventually. PartitionMaxJobsLimit Maximum number of jobs for your job\u2019s partition have been met; job will run eventually. PartitionNodeLimit All nodes assigned to your job\u2019s specified partition are in use; job will run eventually. AssociationCpuLimit All CPUs assigned to your job\u2019s specified association are in use; job will run eventually. AssociationMaxJobsLimit Maximum number of jobs for your job\u2019s association have been met; job will run eventually. AssociationNodeLimit All nodes assigned to your job\u2019s specified association are in use; job will run eventually. <p>A complete list can be found in SLURM's documentation</p> <p>The columns to show can be customized using the <code>--format</code> option, but can also be set with the environment variable <code>SQUEUE_FORMAT</code> to avoid typing it every time. You can always override this to suit your needs in your <code>.bashrc</code> file. The default format is currently:</p> <pre><code>SQUEUE_FORMAT=\"%.12i %.16j %.10u %.10M %.12L %.3C %.6m %.2t %.9P %R\"\n</code></pre> <p>See a full list here.</p>"},{"location":"slurm/jobcontrol/#prevent-pending-job-from-starting","title":"Prevent pending job from starting","text":"<p>Pending jobs can be marked in a \"hold\" state to prevent them from starting <pre><code>scontrol hold &lt;job_id&gt;\n</code></pre></p> <p>To release a queued job from the \u2018hold\u2019 or 'requeued held' states: <pre><code>scontrol release &lt;job_id&gt;\n</code></pre></p> <p>To cancel and rerun (requeue) a particular job: <pre><code>scontrol requeue &lt;job_id&gt;\n</code></pre></p>"},{"location":"slurm/jobcontrol/#cancel-a-job","title":"Cancel a job","text":"<p>With <code>sbatch</code> you won't be able to just hit CTRL+c to stop what's running like you're used to in a terminal. Instead you must use <code>scancel</code>. Get the job ID from <code>squeue --me</code>, then use <code>scancel</code> to cancel a running job, for example: <pre><code>$ scancel &lt;job_id&gt;\n\n# cancel ALL your jobs\n$ scancel --me\n</code></pre></p> <p>If the particular job doesn't stop and doesn't respond, consider using <code>skill</code> instead.</p>"},{"location":"slurm/jobcontrol/#pause-or-resume-a-job","title":"Pause or resume a job","text":"<p>Use <code>scontrol</code> to control your own jobs, for example suspend a running job: <pre><code>$ scontrol suspend &lt;job_id&gt;\n</code></pre></p> <p>Resume again with <pre><code>$ scontrol resume &lt;job_id&gt;\n</code></pre></p>"},{"location":"slurm/jobcontrol/#show-details-about-a-running-or-queued-job","title":"Show details about a running or queued job","text":"<pre><code>scontrol show jobid=&lt;jobid&gt;\n</code></pre> <p>If needed, you can also obtain the batch script used to submit a job: <pre><code>scontrol write batch_script &lt;jobid&gt;\n</code></pre></p>"},{"location":"slurm/jobcontrol/#modifying-job-attributes","title":"Modifying job attributes","text":"<p>Only a few job attributes can be changed after a job is submitted and NOT running yet. These attributes include:</p> <ul> <li>time limit</li> <li>job name</li> <li>job dependency</li> <li>partition or QOS</li> <li>nice value</li> </ul> <p>For example: <pre><code>$ scontrol update JobId=&lt;jobid&gt; timelimit=&lt;new timelimit&gt;\n$ scontrol update JobId=&lt;jobid&gt; partition=zen3\n</code></pre></p> <p>If the job is already running, adjusting the time limit must be done by an administrator.</p>"},{"location":"slurm/jobsubmission/","title":"Job submission","text":"<p>Once you are logged in to one of the login nodes through SSH, there are several ways to request resources and run jobs at different complexity levels through SLURM. Here are the most essential ways for interactive (foreground) and non-interactive (background) use. Usually, you only need to be acquainted with 3 SLURM job submission commands depending on your needs. These are <code>srun</code>, <code>salloc</code>, and <code>sbatch</code>. They all share the exact same options to define trackable resource constraints (\"TRES\" in SLURM parlor, fx number of CPUs, memory, GPU, etc), time limits, email for job status notifications, and many other things, but are made for different use-cases, which will be described below.</p>"},{"location":"slurm/jobsubmission/#interactive-jobs","title":"Interactive jobs","text":"<p>An interactive job is useful for quick testing and development purposes, where you only need resources for a short period of time to experiment with scripts or workflows on minimal test data, before submitting larger batch jobs using <code>sbatch</code> that are expected to run for much longer in the background.</p> <p>To immediately request and allocate resources (once available) and start an interactive shell session directly on the allocated compute node(s) through SLURM, just type <code>salloc</code>:</p> <pre><code>$ salloc\n</code></pre> <p>Here SLURM will find a compute node with the default amount of resources available (which is currently 1CPU, 512MB memory, and a 1-hour time limit) and start the session on the allocated compute node(s) within the requested resource constraints. If you need more resources you need to explicitly ask for it, for example:</p> <pre><code>$ salloc --cpus-per-task 2 --mem 4G --time 0-3:00:00\n</code></pre> <p>Resources will then remain allocated until the shell is terminated with <code>CTRL+d</code>, typing <code>exit</code>, or closing the window. If it takes more than a few seconds to allocate resources, your job might be queued due to a variety of reasons. If so check the <code>REASON</code> codes for the job with <code>squeue</code> from another session.</p> Interactive jobs and CPU efficiency <p>When using an interactive shell through <code>salloc</code> it's important to keep in mind that the allocated resources remain reserved entirely for you until you <code>exit</code> the shell session. So please don't leave it hanging idle for too long if you know you are not going to actively use it, otherwise other users might have needed the resources in the meantime. Furthermore, interactive jobs are usually very inefficient, because the allocated CPU's do absolutely nothing when you are just typing or clicking around. Therefore, interactive jobs will run on the dedicated <code>interactive</code> partition, which is optimized for inefficient jobs.</p> <p>If you just need to run a single command/script in the foreground, it's much better to use <code>srun</code> instead of <code>salloc</code>, which will run things immediately on a compute node instead of first starting an interactive shell. As opposed to <code>salloc</code> the job is terminated immediately once the command/script finishes, which increases CPU utilization:</p> <pre><code>$ srun --cpus-per-task 8 --mem 16G --time 1-00:00:00 mycommand myoptions\n</code></pre> <p>The terminal will be blocked for the entire duration, hence for longer running jobs it's much more convenient to instead write the commands in a script and submit a non-interactive batch job using <code>sbatch</code>, which will run in the background. For shorter commands, you can also instead of <code>srun &lt;options&gt; &lt;command&gt;</code> use the <code>--wrap</code> option to <code>sbatch</code> to submit it as a non-interactive batch job, for example:</p> <pre><code>sbatch --cpus-per-task 2 --mem 4G --time 0-01:00:00 --wrap \"mycommand myoptions\"\n</code></pre> <p><code>srun</code> is sometimes also used to run multiple tasks/steps (parallel processes) from within batch scripts, which can then span multiple compute nodes and run concurrently.</p> Connectivity and interactive jobs <p>Keep in mind that with interactive jobs briefly losing connection to the login-node can result in the job being killed. This is to avoid that resources would otherwise remain blocked due to unresponsive shell sessions hanging until time runs out. If you still see the job in the <code>squeue</code> overview, however, use <code>sattach</code> to reattach to a running interactive job, just remember to append <code>.interactive</code> to the job ID, fx <code>38.interactive</code>.</p>"},{"location":"slurm/jobsubmission/#graphical-gui-apps","title":"Graphical (GUI) apps","text":""},{"location":"slurm/jobsubmission/#from-the-command-line","title":"From the command line","text":"<p>In order to run graphical applications, simply append the <code>--x11</code> option to <code>salloc</code> or <code>srun</code> and run the program. The graphical app will then show up in a window on your own computer, while running inside a SLURM job on the cluster: <pre><code>$ salloc --cpus-per-task 2 --mem 4G --time 0-3:00:00 --x11\n$ command_to_start_graphical_app\n</code></pre></p> <p>It's important to mention that in order for this to work properly, you must first ensure that you have connected to the particular login node using either the <code>ssh -X</code> option, or that you have set the <code>ForwardX11 yes</code> option in your SSH config file, see example here.</p>"},{"location":"slurm/jobsubmission/#from-the-interactive-web-portal-open-ondemand","title":"From the interactive web portal (Open Ondemand)","text":"<p>To run graphical/interactive software, you can also just use the interactive web portal to start a virtual desktop and run it in there. This is especially handy if it needs to run for a long time, because you can log off while it's running and come back later to check the status. Just remember to stop/cancel the job as soon as possible to free up resources for other users. If the particular software is started from the command line, you could simply append <code>mycommand &amp;&amp; scancel &lt;jobid&gt;</code> to the command to automatically stop the job serving the virtual desktop once the command has finished running.</p>"},{"location":"slurm/jobsubmission/#batch-jobs-non-interactive-jobs","title":"Batch jobs (non-interactive jobs)","text":"<p>The most convenient and highly recommended way to run most things is by submitting jobs to the job queue for execution in the background in the form of SLURM batch scripts through the <code>sbatch</code> command. Resource requirements are instead defined by <code>#SBATCH</code> comment-style directives at the top of a shell script, and the script is then submitted to SLURM using a simple <code>sbatch script.sh</code> command. This is ideal for submitting large jobs that will run for many hours or days, but of course also for testing/development work. A SLURM batch script should always contain (in order):</p> <ul> <li>Any number of <code>#SBATCH</code> lines with options defining resource constraints and other options for the subsequent SLURM task(s) to be run.</li> <li>A list of commands to load any required conda environments needed for all tasks. This can also be done separately within any external scripts being executed from the job.</li> <li>The main body of the script/workflow, or call to an external script or program to run within the job.</li> </ul> <p>Submit the batch script to the SLURM job queue using <code>sbatch script.sh</code>, and it will then start once the requested amount of resources are available (also taking into account your past usage and priorities of other jobs etc, all 3 job submission commands do that). If you set the <code>--mail-user</code> and <code>--mail-type</code> arguments you should get a notification email once the job starts and finishes with additional details like how many resources you have actually used compared to what you have requested. This is essential information for future jobs to avoid overbooking and maximize resource utilization of the cluster.</p> <p>You can also simply add <code>#SBATCH</code> lines to any shell script you already have, and also run the script with arguments, so for example instead of <code>bash script.sh -i input -o output ...</code> you can simply run <code>sbatch script.sh -i input -o output ...</code>.</p> ALWAYS check up on running and recently completed jobs <p>The queue time and efficiency of the whole cluster is directly dependent on the average CPU efficiency of all jobs. It is therefore extremely important to ensure that your job uses all the CPU's that you've requested for most of the duration of the job. If not, please cancel the job and submit a new one with fewer CPU's, or adjust the job to use all the CPU's more efficiently. Furthermore, please ALWAYS also check up on the CPU efficiency of recently finished jobs by either checking the stats in job notification emails or by using <code>seff &lt;jobid&gt;</code> and adjust your next submissions accordingly to avoid idle CPU's.</p> Non-interactive job output (<code>stdout</code>/<code>stderr</code> streams) <p>The job is handled in the background by the SLURM daemons on the individual compute nodes, so you won't see any output in the terminal. It will instead be written to the file(s) defined by <code>--output</code> and/or <code>--error</code>. To follow along in real time run for example <code>tail -f job_123.out</code> from a login node.</p>"},{"location":"slurm/jobsubmission/#single-node-single-task-example","title":"Single-node, single-task example","text":"<p>A simple example SLURM <code>sbatch</code> script for a single task could look like this:</p> <pre><code>#!/usr/bin/bash -l\n#SBATCH --job-name=myjob\n#SBATCH --output=job_%j_%x.out\n#SBATCH --cpus-per-task=10\n#SBATCH --mem=10G\n#SBATCH --time=0-04:00:00\n#SBATCH --mail-type=END,FAIL,TIME_LIMIT_90\n#SBATCH --mail-user=abc@bio.aau.dk\n\n# Exit script on the first error\nset -euo pipefail\n\n# Load conda environment to make required software available\nmamba activate minimap2\n\n# Obtain number of CPUs available from the SLURM allocation itself\n# (It's best practice to use the same variable everywhere from here and onwards. If you fx change the resource requirements above it's easy to forget to update it everywhere)\nmax_threads=\"$(nproc)\"\n\n# Run any number of commands as part a full pipeline script or call scripts from elsewhere\nminimap2 -t \"$max_threads\" database.fastq input.fastq &gt; out.file\n</code></pre>"},{"location":"slurm/jobsubmission/#array-jobs","title":"Array jobs","text":"<p>If you need to run the same command or script multiple times with different input files, parameters, or arguments, you can use SLURM arrays. This allows you to submit any number of jobs at once, which can then run simultaneously in parallel across the cluster. By making the unique array ID for each job within the array available with the environment variable <code>SLURM_ARRAY_TASK_ID</code>, you can ensure that each job will run with a different input file, parameter, or argument, etc. Each job within the array will get their respective job array ID appended to the parent job ID, for example <code>&lt;jobid&gt;_[0,1,2,3]</code>, and can be controlled individually. You can also set <code>--mail-type=ARRAY_TASKS</code> to receive notification emails for each task in the array if necessary.</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=myarray_job\n#SBATCH --output=job_%j_%x_%a.out\n#SBATCH --cpus-per-task=10\n#SBATCH --mem=10G\n#SBATCH --time=0-04:00:00\n#SBATCH --mail-type=END,FAIL,TIME_LIMIT_90,ARRAY_TASKS\n#SBATCH --mail-user=abc@bio.aau.dk\n#SBATCH --array=0-8\n\n# Run a script for all files in a folder\nFILES=(/path/to/data/*)\n./my_script ${FILES[$SLURM_ARRAY_TASK_ID]}\n\n# Or run a script with a predefined list of arguments, files, folders, or anything else\nARGS=(0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9)\n./my_script ${ARGS[$SLURM_ARRAY_TASK_ID]}\n</code></pre> Important <p>The <code>bash -l</code> in the top \"shebang\" line is required for the compute nodes to be able to load conda environments correctly.</p>"},{"location":"slurm/jobsubmission/#requesting-one-or-more-gpus","title":"Requesting one or more GPUs","text":"<p>If you need to use one or more GPUs add <code>--gres=gpu:model:x</code> to the <code>sbatch</code>, <code>salloc</code>, or <code>srun</code> commands, where <code>model</code> is one of the available GPU models (see GPU partitions for available models), and <code>x</code> is the number of GPUs to request, for example: <code>--gres=gpu:a10:1</code>. You can also ask for any GPU regardless of model by just excluding it, for example <code>--gres=gpu:1</code>, but note that there can be quite a big difference between GPUs. Please never do CPU work on GPU nodes unless you also need a GPU. Additional details here.</p>"},{"location":"slurm/jobsubmission/#requesting-compute-nodes-with-special-features","title":"Requesting compute nodes with special features","text":"<p>If you need to run your job on compute nodes with special node features, for example if you need local scratch space or a specific CPU model/generation, you can use the <code>--prefer</code> or <code>--constraint</code> options. A list of features for each compute node can be found under Compute node partitions. The <code>--prefer</code> option will only prefer nodes with the specified list of features, but if no such nodes are available it will still run the job on any other available node. The <code>--constraint</code> option will instead require the specified features to be available on the compute node(s), and if no nodes with the required features are available the job will be queued until such nodes become available. For example, to ensure that the job will run on a compute node with the <code>zen5</code> CPU generation and local scratch space, you can use:</p> <pre><code>$ sbatch --constraint=zen5,scratch batchscript.sh\n</code></pre> <p>If you simply want to prefer nodes with the later <code>zen5</code> CPU generation, but don't require it, you can use:</p> <pre><code>$ sbatch --prefer=zen5 batchscript.sh\n</code></pre> <p>Of course these options can be written in the <code>#SBATCH</code> section of batch scripts as well.</p>"},{"location":"slurm/jobsubmission/#how-to-check-up-on-the-resource-utilization-of-running-jobs","title":"How to check up on the resource utilization of running jobs","text":"<p>When jobs start running, it's important to check up on them once in a while to ensure that they run as intended before potentially wasting resources for a long time (especially CPUs). This can be done by starting an interactive shell session within running (non-interactive batch) jobs using the <code>srun</code> command. First, obtain the job ID from the queue using <code>sq</code> (convenient alias for <code>squeue --me</code>):</p> <pre><code>$ sq\n  JOBID   NAME       USER ACCOUNT  TIME  TIME_LEFT CPU MIN_MEM ST PRIO PARTITION NODELIST(REASON)\n2015364 dorado ksa@bio.aa     phn 32:00    2:28:00  64     32G  R  265      zen5 bio-node13\n</code></pre> <p>The job ID is also printed to the terminal when submitting jobs. Then, you can monitor the resource usage of all processes running within the job from the inside in real time using <code>htop</code> or <code>top</code> (and <code>nvidia-smi</code> or <code>nvtop</code> for GPU jobs).</p> <pre><code>$ srun --jobid &lt;jobid&gt; --pty bash\n$ htop\n</code></pre> <p>When using <code>htop</code> you can hit <code>u</code> to filter processes to only show those running by your user, and <code>t</code> for tree view to see the process tree started from each slurm job on the node.</p> <p></p> <p>In this example I have allocated 64 CPUs and 32GB memory for the job and currently keeping about 58 CPUs busy. That is great, but I'm barely using any memory, so the next time I should probably ask for less memory if it stays at this level for the entire duration of the job. Memory is only a maximum though, only peak memory usage matters.</p> <p>Note that if the job spans multiple nodes you have to specify which node to connect to using the <code>--nodelist</code> option. Secondly, only a single interactive shell can be active within the same job allocation at any one time.</p>"},{"location":"slurm/jobsubmission/#most-essential-options","title":"Most essential options","text":"<p>There are plenty of options with the SLURM job submission commands. Below are the most important ones for our current setup and common use-cases. If you need anything else you can start with the SLURM cheatsheet, or refer to the SLURM documentation for the individual commands <code>srun</code>, <code>salloc</code>, and <code>sbatch</code>.</p> Option Default value(s) Description <code>--job-name</code> The name of the script A user-defined name for the job or task. This name helps identify the job in logs and accounting records. <code>--begin</code> <code>now</code> Specifies a start time for the job to begin execution. Jobs won't start before this time. Details here. <code>--output</code>, <code>--error</code> <code>slurm-&lt;jobid&gt;.out</code> Redirect the job's standard output/error (<code>stdout</code>/<code>stderr</code>) to a file, ideally on network storage. All directories in the path must exist before the job can start. By default <code>stderr</code> and <code>stdout</code> are merged into a file <code>slurm-%j.out</code> in the current workdir, where <code>%j</code> is the job allocation number. See filename patterns here. <code>--ntasks-per-node</code> <code>1</code> Specifies the number of tasks to be launched per allocated compute node. <code>--ntasks</code> <code>1</code> Indicates the total number of tasks or processes that the job should execute. <code>--cpus-per-task</code> <code>1</code> Sets the number of CPU cores allocated per task. Required for parallel and multithreaded applications. <code>--mem</code>, <code>--mem-per-cpu</code>, or <code>--mem-per-gpu</code> <code>512MB</code> (per node) Specifies the memory limit per node, or per allocated CPU/GPU. These are mutually exclusive. <code>--nodes</code> <code>1</code> Indicates the total number of compute nodes to be allocated for the job. <code>--nodelist</code> Specifies a comma-separated list of specific compute nodes to be allocated for the job. <code>--exclusive</code> Flag. If set will request exclusive access to a full compute node, meaning no other jobs will be allowed to run on the node. In this case you might as well also use all available memory by setting <code>--mem=0</code>, unless there are suspended jobs on the particular node. Details here. <code>--gres</code> List of \"generic consumable resources\" to use, for example a GPU. Details here. <code>--prefer</code> or <code>--constraint</code> Prefer or require specific node features (see Compute node partitions), respectively. Details here <code>--reservation</code> Allocate resources for the job from the named reservation(s). Can be a comma-separated list of more than one. Details here <code>--chdir</code> Set the working directory of the batch script before it's executed. Setting this using environment variables is not supported. <code>--time</code> <code>0-01:00:00</code> Defines the maximum time limit for job execution before it will be killed automatically. Format <code>DD-HH:MM:SS</code>. Maximum allowed value is that of the partition used. Details here <code>--mail-type</code> <code>NONE</code> Configures email notifications for certain job events. One or more comma-separated values of: <code>NONE</code>, <code>ALL</code>, <code>BEGIN</code>, <code>END</code>, <code>FAIL</code>, <code>REQUEUE</code>, <code>ARRAY_TASKS</code>. <code>TIME_LIMIT_90</code>, <code>TIME_LIMIT_80</code>, and <code>TIME_LIMIT_50</code> might also become handy to be able to extend the time limit before the job is killed. Details here <code>--mail-user</code> Local user A comma-separated list of email addresses where job notifications are sent. <code>--x11</code> <code>all</code> Enable forwarding of graphical applications from the job to your computer. It's required that you have either connected using the <code>ssh -X</code> option or you have set the <code>ForwardX11 yes</code> option in your SSH config file. For <code>salloc</code> or <code>srun</code> only. Details here. <code>--wrap</code> For <code>sbatch</code> only. Submit a command string as a batch script. Details here."},{"location":"slurm/multistep_jobs/","title":"Multi-step jobs","text":"<p>Many steps in a complex workflow will only run on a single thread regardless of whether you've asked for more. This leads to a waste of resources. You can submit separate jobs by writing down the commands in separate shell scripts, then submit them as individual jobs using sbatch with different resource requirements:</p> <p><code>launchscript.sh</code> <pre><code>#!/bin/bash\n\nset -euo pipefail\n\n# Submit the first job step and capture its job ID\nstep1_jobid=$(sbatch step1_script.sh | awk '{print $4}')\n\n# Submit the second job, ensuring it runs only after the first job completes successfully\nstep2_jobid=$(sbatch --dependency=afterok:$step1_jobid step2_script.sh | awk '{print $4}')\n\n# Submit the third/last job, ensuring it runs only after the second job completes successfully\nsbatch --dependency=afterok:$step2_jobid step3_script.sh\n</code></pre></p> <p>Types of Dependencies:  - <code>afterok</code>: The dependent job runs if the first job completes successfully (exit code 0).  - <code>afternotok</code>: The dependent job runs if the first job fails (non-zero exit code).  - <code>afterany</code>: The dependent job runs after the first job completes, regardless of success or failure.  - <code>after:&lt;job_id&gt;</code>: The dependent job starts when the first job begins execution.</p> <p>In this case, using <code>--dependency=afterok</code> ensures that the second job will only start if the first job finishes without errors.</p> <p>Submit using bash not sbatch.</p> <p>This can be repeated as many times as necessary. Any arguments can be passed on to the shell scripts in the exact same way as when invoking them using <code>bash script -i \"some option\" -o \"some other option\"</code> as usual.</p>"},{"location":"slurm/other/","title":"Other commands / FAQ","text":"<p>Below are some nice to know commands with example output and some common problems. This will continously be populated as people ask for certain things. Your question here!</p>"},{"location":"slurm/other/#im-not-allowed-to-submit-jobs","title":"I'm not allowed to submit jobs","text":"<pre><code>salloc: error: Job submit/allocate failed: Invalid account or account/partition combination specified\n</code></pre> <p>This error means you have not been associated with any usage account yet, so you must contact an administrator to add your user to the correct account.</p>"},{"location":"slurm/other/#my-job-is-pending-with-a-requeued-held-status","title":"My job is pending with a \"requeued held\" status","text":"<p>This means something went wrong when the job started on a compute node, so the job went back into the queue to avoid draining the node. It will stay in this state forever until manually restarted. Try running a <code>scontrol release &lt;job_id&gt;</code> and if that doesn't work contact an administrator.</p>"},{"location":"slurm/other/#show-busyfree-cores-for-the-entire-cluster","title":"Show busy/free cores for the entire cluster","text":"<p>Example output (A=allocated, I=idle, O=other, T=total): <pre><code>$ sinfo -o \"%C\"\nCPUS(A/I/O/T)\n620/596/240/1456\n</code></pre></p>"},{"location":"slurm/other/#show-details-about-the-whole-cluster-configuration","title":"Show details about the whole cluster configuration","text":"<pre><code>$ scontrol show config\n</code></pre>"},{"location":"slurm/other/#slurm-environment-variables","title":"SLURM environment variables","text":"<p>SLURM jobs will have a variety of environment variables set within job allocations, which might become handy programmatically. Below is an overview of relevant ones. For all of them refer to the SLURM documentation for input environment variables and output environment variables. Some may not be present for your particular job, so to list only those currently available within a job run for example <code>env | grep -iE 'SLURM|SBATCH'</code>.</p> Variable(s) Description <code>SLURM_ARRAY_TASK_COUNT</code> Total number of tasks in a job array <code>SLURM_ARRAY_TASK_ID</code> Job array ID (index) number <code>SLURM_ARRAY_TASK_MAX</code> Job array's maximum ID (index) number <code>SLURM_ARRAY_TASK_MIN</code> Job array's minimum ID (index) number <code>SLURM_ARRAY_TASK_STEP</code> Job array's index step size <code>SLURM_ARRAY_JOB_ID</code> Job array's master job ID number <code>SLURM_CLUSTER_NAME</code> Name of the cluster on which the job is executing <code>SLURM_CPUS_ON_NODE</code> Number of CPUS on the allocated node <code>SLURM_CPUS_PER_TASK</code> Number of cpus requested per task. Only set if the <code>--cpus-per-task</code> option is specified. <code>SLURM_JOB_ACCOUNT</code> Account name associated of the job allocation <code>SLURM_JOBID</code>, <code>SLURM_JOB_ID</code> The ID of the job allocation <code>SLURM_JOB_CPUS_PER_NODE</code> Count of processors available to the job on this <code>SLURM_JOB_DEPENDENCY</code> Set to value of the <code>--dependency</code> option <code>SLURM_JOB_NAME</code> Name of the job <code>SLURM_NODELIST</code>, <code>SLURM_JOB_NODELIST</code> List of nodes allocated to the job <code>SLURM_NNODES</code>, <code>SLURM_JOB_NUM_NODES</code> Total number of different nodes in the job's resource allocation <code>SLURM_MEM_PER_NODE</code> Takes the value of <code>--mem</code> if this option was specified. <code>SLURM_MEM_PER_CPU</code> Takes the value of <code>--mem-per-cpu</code> if this option was specified. <code>SLURM_NTASKS</code>, <code>SLURM_NPROCS</code> Same as <code>-n</code> or <code>--ntasks</code> if either of these options was specified. <code>SLURM_NTASKS_PER_NODE</code> Number of tasks requested per node. Only set if the <code>--ntasks-per-node</code> option is specified. <code>SLURM_NTASKS_PER_SOCKET</code> Number of tasks requested per socket. Only set if the <code>--ntasks-per-socket</code> option is specified. <code>SLURM_SUBMIT_DIR</code> The directory from which sbatch was invoked <code>SLURM_SUBMIT_HOST</code> The hostname of the computer from which sbatch was invoked <code>SLURM_TASK_PID</code> The process ID of the task being started <code>SLURMD_NODENAME</code> Name of the node running the job script <code>SLURM_JOB_GPUS</code> GPU IDs allocated to the job (if any)."},{"location":"slurm/partitions/","title":"Compute node partitions","text":"<p>The compute nodes are divided into separate partitions based on their hardware configuration. This is to allow that for example CPU's from different manufacturer generations can be set up with a different billing factor to ensure fair usage accounting (newer CPU's are faster), nodes with more memory (per CPU) are only used for jobs that actually require more memory, and GPU nodes are only used for jobs that require GPU's, etc.</p>"},{"location":"slurm/partitions/#automatic-partition-selection","title":"Automatic partition selection","text":"<p>BioCloud is a quite heterogeneous cluster because nodes are purchased at different times, hence their hardware configuration is also different. Furthermore, the number of partitions will only increase in the future as more nodes are added to the cluster at different times, which increases complexity, making it difficult or confusing to submit jobs to the most appropriate partition(s). This can result in an inefficient cluster with longer queue times and wasted computing resources. Therefore, the most appropriate partition for batch jobs is automatically assigned by the SLURM scheduler according to custom logics defined for our specific setup. Manually specifying a partition using the <code>--partition</code> option will have no effect, as it will be overridden. Interactive jobs will always be assigned the <code>interactive</code> partition.</p> <p>The most appropriate partition is determined by several factors, the most significant of which are the requested memory per CPU ratio and any specified node features. Partitions are allocated according to the defined priority tiers shown in the table below, ensuring that newer (and faster) compute nodes are always selected first before older nodes. Secondly, if all general compute nodes happen to be fully allocated, jobs with only modest memory requirements will still be able to run on compute nodes with extra memory instead, if available. Conversely, jobs that require extra memory per CPU will not be able to run on a general compute node as this can result in unavailable CPUs due to fully allocated memory. In certain situations, the automatically assigned partition may not be optimal, in which case manual intervention by an administrator may be necessary and exceptions can be made if it makes sense depending on the situation.</p>"},{"location":"slurm/partitions/#cpu-partitions","title":"CPU partitions","text":"<p>Below is a brief overview of all CPU partitions. Details about the exact CPU model, scratch space and node features for each compute node are listed further down.</p>"},{"location":"slurm/partitions/#overview","title":"Overview","text":"Partition Nodes Total CPUs Total memory Billing factor Priority tier <code>interactive</code> 1 288T 1.5 TB 0.5x - <code>zen5</code> 4 1088T 6.0 TB 1.0x 1st <code>zen3</code> 8 1408T 6.5 TB 0.5x 2nd <code>zen5x</code> 2 576T 4.6 TB 1.5x 3rd <code>zen3x</code> 2 448T 4.0 TB 1.0x 4th TOTAL 17 3872 22.6 TB"},{"location":"slurm/partitions/#the-interactive-partition","title":"The <code>interactive</code> partition","text":"<p>This partition is reserved for short and small interactive jobs, where users can do data analysis, quick testing, and day-to-day work without having to wait for hours or even days due to queue time. Therefore, no batch jobs will be able to run here, and there is a limited amount of resources available to ensure high availability. Ideally, the <code>interactive</code> partition should never be fully utilized. Furthermore, it is optimized for interactive jobs, which are usually very inefficient (e.i. the allocated CPU's do absolutely nothing when you are just typing or clicking around).</p> Hostname CPU model CPUs Memory Scratch space Features <code>bio-node11</code> 2x AMD EPYC 9565 144C / 288T 1.5 TB <code>zen5</code><code>epyc9565</code>"},{"location":"slurm/partitions/#batch-job-partitions","title":"Batch job partitions","text":"<p>These partitions are dedicated to non-interactive and efficient batch jobs that can potentially run for a long time. Some nodes have a higher memory per CPU ratio than others, hence they are separated into different partitions, where those with more memory are prefixed with <code>x</code>. The partitions are otherwise named appropriately according to the generation of AMD EPYC CPUs installed in the nodes.</p> <p><code>zen3</code></p> Hostname CPU model CPUs Memory Scratch space Features <code>bio-node01</code> 2x AMD EPYC 7713 128C / 256T 1.0 TB 3.5 TB NVMe <code>zen3</code><code>epyc7713</code><code>scratch</code> <code>bio-node02</code> 2x AMD EPYC 7552 96C / 192T 0.5 TB <code>zen3</code><code>epyc7552</code> <code>bio-node[03,04,06,07]</code> 2x AMD EPYC 7643 96C / 192T 1.0 TB <code>zen3</code><code>epyc7643</code> <code>bio-node05</code> 2x AMD EPYC 7643 96C / 192T 1.0 TB 18 TB NVMe <code>zen3</code><code>epyc7643</code><code>scratch</code> <p><code>zen3x</code></p> Hostname CPU model CPUs Memory Scratch space Features <code>bio-node08</code> 2x AMD EPYC 7643 96C / 192T 2.0 TB <code>zen3</code><code>epyc7643</code> <code>bio-node09</code> 2x AMD EPYC 7713 128C / 256T 2.0 TB 12.8 TB NVMe <code>zen3</code><code>epyc7713</code> <p><code>zen5</code></p> Hostname CPU model CPUs Memory Scratch space Features <code>bio-node[12-13]</code> 2x AMD EPYC 9565 144C / 288T 1.5 TB <code>zen5</code><code>epyc9565</code> <code>bio-node[16-17]</code> 2x AMD EPYC 9535 128C / 256T 1.5 TB <code>zen5</code><code>epyc9535</code> <p><code>zen5x</code></p> Hostname CPU model CPUs Memory Scratch space Features <code>node[14-15]</code> 2x AMD EPYC 9565 144C / 288T 2.3 TB 12.8 TB NVMe <code>zen5</code><code>epyc9565</code><code>scratch</code>"},{"location":"slurm/partitions/#gpu-partitions","title":"GPU partitions","text":"<p>Nodes in this partition have GPUs installed and should ONLY be used when a GPU is needed for the job. The partition is chosen depending on the GPU model requested using the <code>--gres</code> option to <code>salloc</code>, <code>srun</code>, and <code>sbatch</code> job submission commands. Instructions on how to request a GPU node can be found in the job submission page.</p> <p><code>gpu-a10</code></p> Hostname CPU model CPUs Memory Scratch space GPU Features <code>bio-node10</code> 2x AMD EPYC 7313 32C / 64T 256 GB 3.0 TB NVMe NVIDIA A10 <code>zen3</code><code>epyc7313</code><code>scratch</code><code>a10</code>"},{"location":"slurm/usagereporting/","title":"Usage reporting","text":"<p>Every month users get an automated usage report sent by email. The report contains only basic, but important, information such as total CPU allocation hours and an efficiency summary of all jobs run in the past month. The latter is essential information to minimize wasted resources and thus also the average queue time for all users. For additional data you can use some of the commands below.</p>"},{"location":"slurm/usagereporting/#running-jobs","title":"Running jobs","text":"<p>Use <code>sstat</code> to show the status and live usage accounting information of only running jobs. For batch scripts you need to add <code>.batch</code> to the job ID, for example: <pre><code>$ sstat &lt;job_id&gt;.batch\n</code></pre></p> <p>This will print EVERY metric, so it's nice to select only a few most relevant ones, for example:</p> <pre><code>$ sstat --jobs &lt;job_id&gt;.batch --format=jobid,avecpu,maxrss,ntasks\n</code></pre> Useful format variables Variable Description avecpu Average CPU time of all tasks in job. averss Average resident set size of all tasks. avevmsize Average virtual memory of all tasks in a job. jobid The id of the Job. maxrss Maximum number of bytes read by all tasks in the job. maxvsize Maximum number of bytes written by all tasks in the job. ntasks Number of tasks in a job. <p>For all variables see the SLURM documentation</p>"},{"location":"slurm/usagereporting/#past-jobs","title":"Past jobs","text":"<p>To view the status of past jobs and their usage accounting information use <code>sacct</code>. <code>sacct</code> will return everything accounted for by default which is very inconvenient to view in a terminal window, so the below command will show the most essential information: <pre><code>$ sacct -o jobid,jobname,start,end,NNodes,NCPUS,ReqMem,CPUTime,AveRSS,MaxRSS --user=$USER --units=G -j 138\nJobID           JobName               Start                 End   NNodes      NCPUS     ReqMem    CPUTime     AveRSS     MaxRSS \n------------ ---------- ------------------- ------------------- -------- ---------- ---------- ---------- ---------- ---------- \n138          interacti+ 2023-11-21T10:43:48 2023-11-21T10:43:59        1         16        20G   00:02:56                       \n138.interac+ interacti+ 2023-11-21T10:43:48 2023-11-21T10:43:59        1         16              00:02:56          0          0 \n138.extern       extern 2023-11-21T10:43:48 2023-11-21T10:43:59        1         16              00:02:56      0.00G      0.00G \n</code></pre></p> <p>There is a large number of other options to show, see SLURM docs. If you really want to see everything use for example <code>sacct --long | less -S</code>.</p>"},{"location":"slurm/usagereporting/#reservations","title":"Reservations","text":"<p>Show current reservations in the system and reservation usage of the reservation total <pre><code># show current reservations in the system\n$ sinfo -T\nRESV_NAME       STATE           START_TIME             END_TIME     DURATION  NODELIST\nmaintenance  INACTIVE  2023-12-18T23:00:00  2023-12-20T01:00:00   1-02:00:00  bio-oscloud[02-09]\n\n# show details about all current reservations\n$ scontrol show reservations\nReservationName=amplicon StartTime=2024-11-04T08:00:00 EndTime=2024-11-18T08:00:00 Duration=14-00:00:00\n   Nodes=bio-oscloud03 NodeCnt=1 CoreCnt=192 Features=(null) PartitionName=general Flags=\n     NodeName=bio-oscloud03 CoreIDs=0-191\n   TRES=cpu=192\n   Users=abc@bio.aau.dk Groups=(null) Accounts=(null) Licenses=(null) State=ACTIVE BurstBuffer=(null) Watts=n/a\n   MaxStartDelay=(null)\n\n# show reservation utilization in CPU hours and percent of the reservation total used\n$ sreport reservation utilization -t hourper\n--------------------------------------------------------------------------------\nReservation Utilization 2024-11-04T00:00:00 - 2024-11-04T23:59:59\nUsage reported in TRES Hours/Percentage of Total\n--------------------------------------------------------------------------------\n  Cluster      Name               Start                 End      TRES Name                     Allocated                          Idle \n--------- --------- ------------------- ------------------- -------------- ----------------------------- ----------------------------- \n biocloud  amplicon 2024-11-04T08:00:00 2024-11-05T15:18:55            cpu                  1154(19.20%)                  4858(80.80%) \n</code></pre></p>"},{"location":"slurm/usagereporting/#job-efficiency-summary","title":"Job efficiency summary","text":""},{"location":"slurm/usagereporting/#individual-jobs","title":"Individual jobs","text":"<p>To view the efficiencies of individual jobs use <code>seff</code>, for example:</p> <pre><code>$ seff 2357\nJob ID: 2357\nCluster: biocloud\nUser/Group: &lt;username&gt;\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 96\nCPU Utilized: 60-11:06:29\nCPU Efficiency: 45.76% of 132-02:48:00 core-walltime\nJob Wall-clock time: 1-09:01:45\nMemory Utilized: 383.42 GB\nMemory Efficiency: 45.11% of 850.00 GB\n</code></pre> <p>This information will also be shown in notification emails when jobs finish.</p>"},{"location":"slurm/usagereporting/#multiple-jobs","title":"Multiple jobs","text":"<p>Perhaps a more useful way to use <code>sacct</code> is through the reportseff tool (pre-installed), which can be used to calculate the CPU, memory, and time efficiencies of past jobs, so that you can optimize future jobs and ensure resources are utilized to the max (2TM). For example: <pre><code>$ reportseff -u $(whoami) --format partition,jobid,state,jobname,alloccpus,reqmem,elapsed,cputime,CPUEff,MemEff,TimeEff -S r,pd,s --since d=4\n   Partition     JobID        State                       JobName                    AllocCPUS     ReqMem   Elapsed        CPUTime      CPUEff   MemEff   TimeEff \ninteractive      306282     COMPLETED                     midasok                        2         8G       00:13:07       00:26:14      4.1%    15.4%     10.9%  \n       zen3      306290     COMPLETED           smk-map2db-sample=barcode46             16         24G      00:01:38       00:26:08     90.6%    18.7%     2.7%   \n       zen3      306291     COMPLETED           smk-map2db-sample=barcode47             16         24G      00:02:14       00:35:44     92.8%    18.4%     3.7%   \n       zen3      306292     COMPLETED           smk-map2db-sample=barcode58             16         24G      00:02:32       00:40:32     80.3%    19.0%     4.2%   \n       zen3      306293     COMPLETED           smk-map2db-sample=barcode34             16         24G      00:02:16       00:36:16     78.1%    18.7%     3.8%   \n       zen3      306294     COMPLETED           smk-map2db-sample=barcode22             16         24G      00:02:38       00:42:08     81.1%    19.0%     4.4%   \n       zen3      306295     COMPLETED           smk-map2db-sample=barcode35             16         24G      00:02:26       00:38:56     79.0%    19.0%     4.1%   \n       zen3      306296     COMPLETED           smk-map2db-sample=barcode82             16         24G      00:01:39       00:26:24     68.9%    17.8%     2.8%   \n       zen3      306297     COMPLETED           smk-map2db-sample=barcode70             16         24G      00:02:04       00:33:04     76.0%    19.5%     3.4%   \n       zen3      306298     COMPLETED           smk-map2db-sample=barcode94             16         24G      00:01:44       00:27:44     70.1%    17.9%     2.9%   \n       zen3      306331     COMPLETED           smk-map2db-sample=barcode59             16         24G      00:04:00       01:04:00     87.2%    19.6%     6.7%   \n       zen3      306332     COMPLETED           smk-map2db-sample=barcode83             16         24G      00:02:13       00:35:28     76.3%    19.0%     3.7%   \n       zen3      306333     COMPLETED           smk-map2db-sample=barcode11             16         24G      00:02:49       00:45:04     81.6%    19.4%     4.7%   \n       zen3      306334     COMPLETED           smk-map2db-sample=barcode23             16         24G      00:03:33       00:56:48     61.6%    19.0%     5.9%   \n       zen3      306598     COMPLETED      smk-mapping_overview-sample=barcode46         1         4G       00:00:09       00:00:09     77.8%     0.0%     1.5%   \n       zen3      306601     COMPLETED      smk-mapping_overview-sample=barcode82         1         4G       00:00:09       00:00:09     77.8%     0.0%     1.5%   \n       zen3      306625     COMPLETED      smk-mapping_overview-sample=barcode94         1         4G       00:00:07       00:00:07     71.4%     0.0%     1.2%   \n       zen3      306628     COMPLETED      smk-concatenate_fastq-sample=barcode71        1         1G       00:00:07       00:00:07     71.4%     0.0%     1.2%   \n       zen3      306629     COMPLETED      smk-concatenate_fastq-sample=barcode70        1         1G       00:00:07       00:00:07     71.4%     0.0%     1.2%   \n       zen3      306630     COMPLETED      smk-concatenate_fastq-sample=barcode34        1         1G       00:00:07       00:00:07     57.1%     0.0%     1.2%   \n</code></pre></p> <p>In the example above, way too much memory was requested for all the jobs in general, and also the time limits were way too long. The most important is the CPU efficiency, however, which was generally good except for one job, but it was a very small job.</p>"},{"location":"slurm/usagereporting/#usage-reports","title":"Usage reports","text":"<p><code>sreport</code> can be used to summarize usage in many different ways, below are some examples.</p>"},{"location":"slurm/usagereporting/#account-usage-by-user","title":"Account usage by user","text":"<pre><code>$ sreport cluster AccountUtilizationByUser format=account%8,login%23,used%10 -t hourper start=\"now-1week\"\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2024-03-13T12:00:00 - 2024-03-19T23:59:59 (561600 secs)\nUsage reported in CPU Hours/Percentage of Total\n--------------------------------------------------------------------------------\n Account                   Login               Used \n-------- ----------------------- ------------------ \n    root                             154251(63.71%) \n    root              &lt;username&gt;      37176(15.35%) \n     jln                                3988(1.65%) \n     jln              &lt;username&gt;        2010(0.83%) \n     jln              &lt;username&gt;           0(0.00%) \n     jln              &lt;username&gt;        1978(0.82%) \n     kln                                   7(0.00%) \n     kln              &lt;username&gt;           7(0.00%) \n      ma                               13460(5.56%) \n      ma              &lt;username&gt;        6152(2.54%) \n      ma              &lt;username&gt;        2504(1.03%) \n      ma              &lt;username&gt;        3710(1.53%) \n      ma              &lt;username&gt;         963(0.40%) \n      ma              &lt;username&gt;         131(0.05%) \n      md                              52921(21.86%) \n      md              &lt;username&gt;       18690(7.72%) \n      md              &lt;username&gt;       22443(9.27%) \n      md              &lt;username&gt;       11788(4.87%) \n     mms                               17036(7.04%) \n     mms              &lt;username&gt;         600(0.25%) \n     mms              &lt;username&gt;       16436(6.79%) \n     phn                                6799(2.81%) \n     phn              &lt;username&gt;         114(0.05%) \n     phn              &lt;username&gt;          31(0.01%) \n     phn              &lt;username&gt;        6654(2.75%) \n     phn              &lt;username&gt;           0(0.00%) \n     sss                               22081(9.12%) \n     sss              &lt;username&gt;       22081(9.12%) \nstudents                                 638(0.26%) \nstudents              &lt;username&gt;         638(0.26%) \n</code></pre>"},{"location":"slurm/usagereporting/#user-usage-by-account","title":"User usage by account","text":"<pre><code>$ sreport cluster UserUtilizationByAccount format=login%23,account%8,used%10 -t hourper start=\"now-1week\"\n--------------------------------------------------------------------------------\nCluster/User/Account Utilization 2024-03-13T12:00:00 - 2024-03-19T23:59:59 (561600 secs)\nUsage reported in CPU Hours/Percentage of Total\n--------------------------------------------------------------------------------\n                  Login  Account               Used \n----------------------- -------- ------------------ \n             &lt;username&gt;     root      37176(15.35%) \n             &lt;username&gt;       md       22443(9.27%) \n             &lt;username&gt;      sss       22081(9.12%) \n             &lt;username&gt;       md       18690(7.72%) \n             &lt;username&gt;      mms       16436(6.79%) \n             &lt;username&gt;       md       11788(4.87%) \n             &lt;username&gt;      phn        6654(2.75%) \n             &lt;username&gt;       ma        6152(2.54%) \n             &lt;username&gt;       ma        3710(1.53%) \n             &lt;username&gt;       ma        2504(1.03%) \n             &lt;username&gt;      jln        2010(0.83%) \n             &lt;username&gt;      jln        1978(0.82%) \n             &lt;username&gt;       ma         963(0.40%) \n             &lt;username&gt; students         638(0.26%) \n             &lt;username&gt;      mms         600(0.25%) \n             &lt;username&gt; compute+         145(0.06%) \n             &lt;username&gt;       ma         131(0.05%) \n             &lt;username&gt;      phn         114(0.05%) \n             &lt;username&gt;      phn          31(0.01%) \n             &lt;username&gt;      kln           7(0.00%) \n             &lt;username&gt;      phn           0(0.00%) \n             &lt;username&gt;      jln           0(0.00%) \n</code></pre>"},{"location":"slurm/usagereporting/#top-users","title":"Top users","text":"<pre><code>$ sreport user top topcount=20 format=login%21,account%8,used%10 start=\"now-1week\" -t hourper\n--------------------------------------------------------------------------------\nTop 20 Users 2024-03-13T11:00:00 - 2024-03-19T23:59:59 (565200 secs)\nUsage reported in CPU Hours/Percentage of Total\n--------------------------------------------------------------------------------\n                Login  Account               Used \n--------------------- -------- ------------------ \n          &lt;username&gt;      root      37176(15.26%) \n          &lt;username&gt;        md       22698(9.32%) \n          &lt;username&gt;       sss       22082(9.06%) \n          &lt;username&gt;        md       18850(7.74%) \n          &lt;username&gt;       mms       16436(6.75%) \n          &lt;username&gt;        md       12038(4.94%) \n          &lt;username&gt;       phn        6654(2.73%) \n          &lt;username&gt;        ma        6167(2.53%) \n          &lt;username&gt;        ma        3780(1.55%) \n          &lt;username&gt;        ma        2504(1.03%) \n          &lt;username&gt;       jln        2383(0.98%) \n          &lt;username&gt;       jln        1978(0.81%) \n          &lt;username&gt;        ma         963(0.40%) \n          &lt;username&gt;  students         648(0.27%) \n          &lt;username&gt;       mms         600(0.25%) \n          &lt;username&gt;        kt         146(0.06%) \n          &lt;username&gt;        ma         133(0.05%) \n          &lt;username&gt;       phn         114(0.05%) \n          &lt;username&gt;       kln          98(0.04%) \n          &lt;username&gt;       phn          31(0.01%) \n</code></pre>"},{"location":"software/conda/","title":"Conda environments","text":"<p>Conda is an open-source package management system and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs, and updates packages and their dependencies, where a sophisticated dependency solver allows installing multiple tools into the same environment at once without introducing conflicts between required versions of the individual dependencies. This is ideal for scientific projects where reproducibility is key - you can simply create a separate conda environment for each individual project.</p> <p>Conda was initially created for Python packages but it can package and distribute software for any language. Conda also doesn't require elevated privileges allowing users to install anything with ease. Most tools are already available in the default Anaconda repository, but other community-driven channels like bioconda allow installing practically anything. In comparison to containers, Conda is a dependency manager at the Python package level, while containers also manage operating system dependencies at the base operating system level, hence containers and conda environments are often used together to ensure complete reproducibility and portability, see for example the biocontainers.pro project.</p> <p>Cheatsheet here</p> It is recommended to use mamba instead of conda <p>As <code>conda</code> is notoriously slow, it is recommended to instead use the <code>mamba</code> command, which is written in C++ and is generally much faster. <code>mamba</code> is a drop-in replacement with identical sub-commands, so the syntax and usage remains the same. While recent versions of <code>conda</code> have adopted the much faster <code>libmamba</code> environment solver from <code>mamba</code> by default, many other functions can still be slow, for example the shell initialization which happens on login. Note that <code>conda</code> and <code>mamba</code> both use the same package cache, which can result in permission issues if you use both, so it's best to stick to one of them.</p> Activating conda environments in non-interactive batch jobs <p>In non-interactive batch scripts it is important to remember to set <code>bash -l</code> in the top \"shebang\" line for the compute nodes to be able to load conda environments correctly, see example here.</p>"},{"location":"software/conda/#creating-an-environment","title":"Creating an environment","text":"<p>To install software through conda, it must always be done in an environment. Conda itself is already installed and configured on BioCloud, so you don't need to install it first. To create an environment and install some software in it, run for example:</p> <pre><code># create+activate+install\nmamba create -n myproject\nmamba activate myproject\nmamba install -c bioconda somepkg1=1.0 somepkg2=2.0\n\n# or in one command\nmamba create -n myproject -c bioconda somepkg1 somepkg2\n</code></pre> <p>Make sure to add the required conda channels using <code>-c &lt;channel&gt;</code> from which to install the software. Usually the <code>bioconda</code> and <code>conda-forge</code> channels are all you need.</p> <p>The best practice is to always note down all packages including versions used in projects before you forget things to ensure reproducibility. You can always export an activated environment created previously and dump the exact versions used into a YAML file with <code>mamba env export &gt; requirements.yml</code>. The file could for example look like this:</p> <p>requirements.yml <pre><code>name: myproject\nchannels:\n  - bioconda\ndependencies:\n - minimap2=2.26\n - samtools=1.18\n</code></pre></p> <p>To create an environment from the file in the future simply run <code>mamba env create -f requirements.yml</code>.</p> Note <p>When you export a conda environment to a file the file may also contain a host-specific <code>prefix</code> line, which should be removed if you or someone else need to run it elsewhere.</p> <p>To use the software installed in the environment remember to activate the environment first using <pre><code>mamba activate myproject\n</code></pre></p> <p>List available environments with <pre><code>mamba env list\n</code></pre></p>"},{"location":"software/conda/#installing-packages-using-pip-within-conda-environments","title":"Installing packages using pip within conda environments","text":"<p>Software that can only be installed with pip have to be installed in a Conda environment by using pip inside the environment. While issues can arise, per the Conda guide for using pip in a Conda environment, there are some best practices to follow to reduce their likelihood:</p> <ul> <li>Use pip only after conda package installs</li> <li>Use conda environments for isolation (Don't perform pip installs in the <code>base</code> environment)</li> <li>Recreate the entire environment if changes are needed after pip packages have been installed</li> <li>Use <code>--no-cache-dir</code> with any <code>pip install</code> commands</li> </ul> <p>After activating the conda environment an install command would look like the following: <pre><code>$ python3 -m pip install &lt;package&gt; --no-cache-dir\n</code></pre></p> <p>If you then export the conda environment to a YAML file using <code>mamba env export &gt; requirements.yml</code>, software dependencies installed using pip should show under a separate <code>- pip:</code> field, for example: <pre><code>name: myproject\nchannels:\n  - bioconda\ndependencies:\n - minimap2=2.26\n - samtools=1.18\n - pip:\n   - virtualenv==20.25.0\n</code></pre></p> <p>Be aware that specific versions are specified using double <code>==</code> with <code>pip</code> dependencies.</p>"},{"location":"software/conda/#r-and-installing-r-packages-within-conda-environments","title":"R and installing R packages within conda environments","text":"<p>Use the notation <code>r-{package}</code> to install R and required R packages within an environment, see the list of packages here. Alternatively using renv is highly recommended for project reproducibility and portability if you need to install many packages.</p>"},{"location":"software/conda/#vs-code-and-conda-environments","title":"VS Code and conda environments","text":"<p>To ensure VS Code uses for example R and Python installations in conda environments, first install your preferred extensions, then make a file <code>.vscode/settings.json</code> in the current project folder and write for example: <pre><code>{\n  \"r.rterm.linux\": \"${userHome}/.conda/envs/myproject/bin/R\",\n  \"python.defaultInterpreterPath\": \"${userHome}/.conda/envs/myproject/bin/python\"\n}\n</code></pre></p> <p>You can also place the <code>settings.json</code> file at <code>$HOME/.config/Code/User/settings.json</code> instead to make the settings apply for all projects. If any <code>.vscode/settings.json</code> files are present in individual project folders, they will take precedence over <code>$HOME/.config/Code/User/settings.json</code>.</p> <p>Read more details here.</p>"},{"location":"software/containers/","title":"Containers","text":"<p>Containers provide a convenient and portable way to package and run applications in a completely isolated and self-contained environment, making it easy to manage dependencies and ensure complete reproducibility and portability. Compared to conda environments or software modules containers are always based on a base operating system image, usually Linux, ensuring that even the operating system is under control. Once a container is built and working as intended, it will run exactly the same forever, whereever, and is therefore the best way to bundle and distribute production-level workflows. By containerizing the application platform and its dependencies, differences in OS distributions and underlying infrastructure are abstracted away completely. Linux containers allow users to:</p> <ul> <li>Use software with complicated dependencies and environment requirements</li> <li>Run an application container from the Sylabs Container Library, Docker Hub, or from self-made images from the GitHub container registry</li> <li>Use a package manager (like apt or yum) to install software without changing anything on the host system or require elevated privileges</li> <li>Run an application that was built for a different distribution of Linux than the host OS</li> <li>Run the latest released software built for newer Linux OS versions than that present on HPC systems</li> <li>Archive an analysis for long-term reproducibility and/or publication</li> </ul>"},{"location":"software/containers/#singularityapptainer","title":"Singularity/Apptainer","text":"<p>Singularity/Apptainer is a tool for running software containers on HPC systems, but is made specifically with scientific computing in mind. Singularity allows running Docker and any other OCI-based container natively and is a replacement for Docker on HPC systems. Singularity has a few extra advantages:</p> <ul> <li>Security: a user in the container is the same user with the same privileges/permissions as the one running the container, so no privilege escalation is possible</li> <li>Ease of deployment: no daemon running as root on each node, a container is simply an executable</li> <li>Ability to run workflows that require MPI and GPU support</li> </ul>"},{"location":"software/containers/#building-container-images","title":"Building container images","text":"<p>You can build containers using <code>apptainer build</code>, which will produce a <code>.sif</code> file with everything included. You can also build containers externally by:  - using your own system (laptop/workstation) where you have root/elevated privileges to install Singularity or Docker and build containers  - using a free cloud container build service like https://cloud.sylabs.io or https://hub.docker.com/  - by publishing a <code>Dockerfile</code> to a GitHub repository and use GitHub actions to build and publish the container to the GitHub container registry</p> <p>Then transfer the container image file(s) to BioCloud or publish it to a public container registry and pull it using <code>apptainer pull</code>.</p>"},{"location":"software/containers/#bundling-conda-environments-in-a-container","title":"Bundling conda environments in a container","text":"<p>cotainr is perhaps the easiest way to bundle conda environments inside an apptainer container by simply giving a path to an environment yaml file (see the conda page) and choosing a base image from for example docker hub:</p> <pre><code>cotainr build --base-image docker://ubuntu:22.04 --conda-env condaenv.yml myproject.sif\n</code></pre> <p>This will produce a single file anyone can use anywhere apptainer is installed, regardless of platform and local differences in setup, etc.</p>"},{"location":"software/containers/#pre-built-container-images","title":"Pre-built container images","text":"<p>Usually it's not necessary to build a container yourself unless you want to customize things in detail, since there are plenty of pre-built images already available that work straight of the box. For bioinformatic software the community-driven project biocontainers.pro should have anything you need, and if not - you can contribute! If you need a container with multiple tools installed see multi-package containers.</p>"},{"location":"software/containers/#running-a-container","title":"Running a container","text":"<pre><code># pull a container\n$ apptainer pull ubuntu_22.04.sif docker://ubuntu:22.04\n\n# run a container with default options\n$ apptainer run ubuntu_22.04.sif yourcommand --someoption somefile\n\n# start an interactive shell within a container\n$ apptainer shell ubuntu_22.04.sif\n</code></pre>"},{"location":"software/containers/#binding-mounting-folders-from-the-host-to-the-container","title":"Binding (mounting) folders from the host to the container","text":"<p>You almost always need to bind/mount a folder from the host machine to the container, so that it's available inside the container for input/output to the particular tool you need to use. With Singularity/Apptainer the <code>/tmp</code> folder, the current folder, and your home folder are always mounted by default. Sometimes personal configuration files within your home folder may interfere with whatever is installed and configured within the container (for example conda), so it can sometimes be necessary to avoid mounting your home folder by using <code>--no-home</code>. To mount additional folders use <code>-B</code>, for example: <pre><code># Bind with the same path inside the container as on the host\napptainer run -B /databases ubuntu_22.04.sif yourcommand --someoption somefile\n\n# Bind at a different path inside the container\napptainer run -B /databases:/some/other/path/databases ubuntu_22.04.sif yourcommand --someoption somefile\n\n# Binding multiple folders at once\napptainer run -B /databases -B /raw_data -B /projects ubuntu_22.04.sif yourcommand --someoption somefile\n\n# You can also specify mounts setting the APPTAINER_BIND variable before running a container\nexport APPTAINER_BIND=\"/raw_data,/databases,/home,/projects\"\napptainer run ubuntu_22.04.sif yourcommand --someoption somefile\n</code></pre></p> <p>For additional guidance see the Apptainer usage guide. If you need to use a GPU with apptainer use the <code>--nvccli</code> flag, not <code>--nv</code>.</p>"},{"location":"software/containers/#docker-containers","title":"Docker containers","text":"<p>Docker itself is not supported directly for non-admin users due to security and compatibility issues with our user authentication mechanism, but you can instead just run them through apptainer by prepending <code>docker://</code> to the container path, see this page.</p>"},{"location":"software/other/","title":"Other software","text":"<p>In rare cases, some software tools are not readily available through conda or biocontainers, or they are problematic to get to run properly. Below are some guides on how to run some them. Most are installed under <code>/software/lib/biocloud-software</code>.</p>"},{"location":"software/other/#arb-7","title":"ARB 7","text":"<p>ARB version 6.0 can be installed through conda from the bioconda channel, however it is not maintained anymore, and the latest version 7.0 is not available. To run version 7.0 on BioCloud, you need to run a wrapper script that runs ARB 7.0 from a custom built container image. You can do that through either a virtual desktop started from the web portal, or through an interactive shell session using <code>salloc</code> from a login node. ARB 7 is then available by simply typing <code>arb7</code> in the terminal.</p>"},{"location":"software/other/#clc","title":"CLC","text":"<p>CLC version 25 is currently only installed on (and licensed to) the <code>bio-node11</code> node in the <code>interactive</code> partition (currently the only node there). You should be able to find CLC in the menus when starting a virtual desktop.</p>"},{"location":"software/other/#alphafold","title":"AlphaFold","text":"<p>AlphaFold is quite complex to install and run. It runs within a container, but it must be started using a python wrapper script. It has been bundled up under <code>/software/lib/biocloud-software/alphafold_singularity/</code>. Just Copy the SLURM sbatch script from <code>/software/lib/biocloud-software/alphafold_singularity/sbatch_example.sh</code> and adjust to suit your needs. <code>AlphaFold</code> benefits from GPU-accelerated servers, but can also run on regular CPUs depending on the size of the input data.</p>"},{"location":"software/other/#esmfold","title":"ESMFold","text":"<p>ESMFold can be run through a custom-built container by simply typing <code>esm-fold</code>.</p>"},{"location":"storage/bestpractices/","title":"Best practices with Ceph storage","text":""},{"location":"storage/bestpractices/#moving-large-amounts-of-data-around","title":"Moving large amounts of data around","text":"<p>If you need to move large amounts of data (or numerous files at once regardless of size) it will happen in an instant regardless of the size if you make sure you are doing it within the same mount point, for example from <code>/projects/xxx</code> to <code>/projects/yyy</code>. However, if you need to move things across mount points, for example from <code>/home/xxx</code> to <code>/projects/yyy</code> please ask an administrator to do it for you, since moving between mount points will happen over the network and cause unneccessary traffic and load on the storage cluster, let alone be very slow. An administrator can move anything instantly anywhere on the storage cluster.</p> Use symbolic links instead of copying data <p>To avoid data duplication and unnecessary use of storage space, please use symbolic links (using the <code>ln -s</code> command) instead of copying data (using the <code>cp</code> command) whenever possible. This is especially important when working with large files. Symbolic links are simply pointers to the original file.</p>"},{"location":"storage/bestpractices/#avoid-numerous-small-files-at-all-costs","title":"Avoid numerous small files at all costs","text":"<p>Whereever possible always try to write fewer but larger files instead of many small ones. The Ceph storage cluster uses a file metadata cache server (MDS server) that holds an entry for every single open file, so the cache memory usage is directly proportional to the number of open files (being accessed in any way). If there are too many open files (across ALL connected clients/nodes at once) the metadata server will ask clients to release some pressure on the cache and if they fail to do so in time they will simply be evicted without notice (meaning the Ceph cluster will force an unmount to protect itself). This will happen on any node that tips the iceberg and is thus not a result of the exact storage actions of individual clients, but rather that of all nodes connected to the Ceph cluster at any one time. See Ceph best usage practices for additional details.</p>"},{"location":"storage/bestpractices/#dont-kill-processes-too-hard","title":"Don't kill processes too hard","text":"<p>When a process is killed using <code>kill -9</code> (<code>SIGKILL</code> signal code), CephFS will not have the chance to properly close any open file handles, which can leave the Ceph MDS server with stale entries in its metadata cache. Whenever a client accesses a file, the Ceph MDS issues so called capabilities for the file (an inode, to be technically correct). The MDS then waits for the client to release back these capabilities when it's finished with the file, to signal that other clients can now use the capabilities for the file. If a process is killed too hard, the client may never release the capabilities again, which causes serious trouble on the MDS, and the only solution is rebooting the client or forcing an unmount, which will impact other jobs running on the node. If this happens too often, the MDS server will eventually run out of memory and start evicting clients as described above, but it can also lead to dead-locked folders, that are unaccessible until ALL nodes that have been accessing the particular folder recently are rebooted (see this). Therefore, if you ever happen to need to kill processes, just use <code>kill</code> (<code>SIGTERM</code> signal code) and never <code>kill -9</code>, which will allow the Ceph IO processes to catch the signal and close any open file handles properly before exiting.</p>"},{"location":"storage/bestpractices/#avoid-using-ls-l","title":"Avoid using <code>ls -l</code>","text":"<p>When listing directories, it's common to use <code>ls -l</code> to list things vertically, however this will also request various other information like permissions, file size, owner, group, access time etc. This will burden the metadata servers, especially if used in loops in scripts on many files, so if you don't need all this extra information and just want to list the contents vertically instead of horizontally, just use <code>ls -1</code> instead and make that a habit. Likewise, don't use <code>stat</code> on many files if not neccessary.</p>"},{"location":"storage/bestpractices/#obtaining-the-total-size-of-folders","title":"Obtaining the total size of folders","text":"<p>To obtain the total disk space used of all files inside a folder it's common to use the <code>du -sh /some/folder</code> command. Doing this at a large folder is quite similar to performing a DDoS attack on the Ceph storage cluster, so please never use <code>du</code> on folders, only on individual files. It will likely never finish anyways if the folder contains many files. The best way to obtain the size of a folder is to instead obtain the information in the form of storage quota attributes directly from the Ceph metadata servers using the custom <code>storagequota</code> command as demonstrated below, which is both instant and will not cause any stress on the cluster:</p> <pre><code># home folder\n$ storagequota\nStorage quota used for '/home/bio.aau.dk/abc': 3.46TB / 9.09TB (38.06%)\n\n# specific folder\n$ storagequota /projects\nStorage quota used for '/projects': 397.54TB\n\n# multiple folders, sorted by size\nstoragequota /projects/* | sort -rt ':' -k2 -n | head -n 5\nStorage quota used for '/projects/microflora_danica':       208.40TB\nStorage quota used for '/projects/MiDAS':       125.75TB\nStorage quota used for '/projects/NNF_AD_2023':     30.66TB\nStorage quota used for '/projects/glomicave':       8.84TB\nStorage quota used for '/projects/dark_science':    7.19TB\n</code></pre> <p>The <code>storagequota</code> command is simply a convenient wrapper script around the <code>getfattr</code> command, which retrieves attributes of files and folders directly from the Ceph MDS servers. It only retrieves the size of the folder, but there are also a few other attributes that may be of interest, for example the number of files, see examples below.</p> <pre><code>$ getfattr -n ceph.dir.rfiles /projects \ngetfattr: Removing leading '/' from absolute path names\n# file: projects\nceph.dir.rfiles=\"219203126\"\n</code></pre> Additional Ceph attributes Attribute Explanation ceph.dir.entries ceph.dir.files Number of files in folder (non-recursive) ceph.dir.subdirs Number of subdirs (non-recursive) ceph.dir.rentries ceph.dir.rfiles Number of files in folder (recursive) ceph.dir.rsubdirs Number of folders in folder (recursive) ceph.dir.rbytes Size of folder (recursive) ceph.dir.rctime Recently changed time (non-recursive) <p>There are no descriptions on these in the Ceph documentation or anywhere else, I've simply found them in the Ceph source code! This is all there is.</p>"},{"location":"storage/intro/","title":"Mount points","text":"<p>All nodes are connected to a 2PB CephFS network storage cluster through a few different mount points for different purposes. The same mount points are used everywhere, so everything will have the same locations regardless of which node you are accessing from.</p> <p>It is important to stress that the storage should be considered as temporary working storage only. It is NOT a long-term storage solution for cold data, so please clean up after yourself immediately after each job! See the storage policy for more details.</p> Data is NOT backed up <p>Data is stored with 3x replication across multiple disks and storage nodes to protect against data loss or corruption due to hardware and/or disk failures. The data is NOT backed up anywhere, however, which means it's not protected against human errors. Therefore, if you delete data - it's gone forever and cannot be restored! </p> Max file size <p>There is a max file size of 4TB on the CephFS network storage cluster, which cannot be increased, details here.</p>"},{"location":"storage/intro/#mount-points_1","title":"Mount points","text":"Mount point Contents <code>/home</code> Data for individual projects and generally most things should go here. <code>/projects</code> Data for shared projects where multiple people work on the same project(s) should go here. <code>/databases</code> (read-only) Various databases required for bioinformatic tools. <code>/raw_data</code> Raw data archive (mostly from DNA sequencing). Large databases <p>To avoid data duplication and unnecessary use of storage space, please always ask an administrator to download large databases (100GB+), so that everyone else can also benefit from them. Do NOT store large databases in your home or project folders, unless you need to modify or customize them.</p>"},{"location":"storage/intro/#organization-and-naming-of-data","title":"Organization and naming of data","text":"<p>At a university people come and go all the time due to time-limited employments. It is therefore important to organize and name things conveniently, especially witin shared folders, to be able to easily identify who is responsible for what when people leave the university. Therefore, please always organize EVERYTHING under the <code>/raw_data/</code> and <code>/projects</code> mount points with the top-level structure:</p> <p><code>/&lt;mountpoint&gt;/PI/project/(sub-project)</code></p> <p>where PI is an acronym or the initials of the principal investigator (or research group leader) for the project, project is the full name of the project, and sub-project is optional and can be used if there are multiple sub-projects within a larger project.</p> <p>Secondly, naming folders clearly and unambigously is very important, so please avoid using acronyms and abbreviations that only make sense to you. Instead, use full user- or project names, preferably including a date or at least a year, so that it's always clear who is responsible for what. For example:</p> <pre><code>/projects/John_Doe/2025-Microbiome_of_Danish_Water_Systems\n/raw_data/Jane_Smith/Metagenomics_of_Soil_Fungi\n</code></pre>"},{"location":"storage/local/","title":"Local scratch and temporary space","text":"<p>Each job will always have a separate and entirely private virtual mount point for temporary data (default location is usually <code>/tmp</code> or <code>/var/tmp</code>). The folder will automatically be cleared after each job, but only when the last job on the node for a particular user finishes, so that multiple jobs on the same node can work on the same files. On compute nodes with extra local scratch space it will simply be mounted there instead, so that there is a lot more space available for temporary files on those nodes (refer to compute node partitions).</p> <p>For jobs requiring heavy I/O where large amounts of temporary data needs to be written, it's important to avoid using the network storage and instead write to a local harddrive. This will avoid overburdening the network (which other jobs also need) and the storage cluster itself, but it can also be much faster in some cases. When deciding whether you need local scratch space, both the size of the data as well as the number of files are important, however the ladder is by far the most important because it has the biggest impact on the performance of the storage cluster overall, which can impact every other user.</p> <p>To use local scratch space, you must first ensure to submit your job(s) using the <code>--constraint scratch</code> option to the <code>srun</code>, <code>sbatch</code>, and <code>salloc</code> commands, which will ensure that your job is run on a compute node with local scratch space available. Then, simply ensure that your job writes temporary files to the <code>/tmp</code> folder. The <code>/tmp</code> folder is managed by the SLURM scheduler and is private to each user and their jobs, so you can write whatever you want there without worrying about other jobs overwriting your files.</p> <p>If you would need more space for temporary data on compute nodes that have no extra local scratch space, or you need even more temporary space than there's available on local scratch space, it's possible to place it on the Ceph network storage as well. However, if you choose to do so, please see the best practices below. It can simply be done by for example setting the environment variable <code>TMPDIR</code> early in the batch script by adding a line, fx <code>export TMPDIR=${HOME}/tmp</code>. Ensure no conflicts can occur within the folder(s) if you run multiple jobs on multiple different compute nodes at once.</p> Warning <p>All data owned by your user anywhere on local scratch space on a particular compute node is automatically deleted after the last SLURM job run by your user finishes. Therefore, ensure that any output files generated from jobs are moved to one of the network storage mount points as a last step, otherwise they will be lost.</p>"},{"location":"storage/policy/","title":"Storage policy","text":"<p>The Ceph network storage is NOT a long-term storage or backup solution. The storage cluster is to be considered a temporary working storage only, so please clean up after yourself immediately after each job! You will likely forget about it very quickly after you've done your work, so make it a habit to cleanup shortly after every job. Once studies are published, delete all the data and ensure that for example raw DNA sequencing data is uploaded to public archives and that the corresponding code to produce results in the studies is available somewhere, preferably in a GitHub repository.</p> <p>Furthermore, if you are no longer employed or studying at AAU, you have the sole responsibility to pass on any data that should be saved for longer to other users, for example your PI, before you leave. Any data that is left behind where it is impossible to identify who is responsible for it will be deleted without notice, so please ensure that everything is organized properly under <code>/projects/PI/project/(sub-project)</code> as described here.</p> <p>Lastly: Your data - your responsibility!</p>"},{"location":"storage/sharedfolders/","title":"Shared folders","text":"<p>If you need to give other users write access to a file/folder that you own, you need to set the group ownership of the folder to the <code>bio_server_users@bio.aau.dk</code> group and set the setGID bit on folders (to ensure child files/folders will inherit the ownership of a parent folder), see the example below. This will give everyone with access to the BioCloud servers full control of the files. If you only want a specific group of people to have write access, there is only one way to do that, which is to contact the university IT services to create an email address group for the specific users, and then follow the same steps below, but instead use the new email of that group.</p> <p>Never use <code>chmod 777</code>. It's a major security weakness that can allow anyone (even users outside of the university authentication system, human or not) to do anything they want with a file/folder and potentially place hidden payload there. Folders will be scanned regularly for insecure permissions and corrected without notice. Only <code>owner</code> and <code>group</code> should have the execute bit set, never <code>other</code>. See permissions in Linux to learn about file and folder permissions on Linux systems.</p> <pre><code>folder=\"some_folder/\"\n\n# create the folder if it doesn't exist already\nmkdir -p \"$folder\"\n\n# set group ownership\nchown -R :bio_server_users@bio.aau.dk \"$folder\"\n\n# If there are already files with weak permissions, correct them with:\nchmod -R o-x \"$folder\"\n\n# Set correct permissions on all subfolders:\nfind \"$folder\" -type d -exec chmod 775 {} \\;\n\n# set the setGID sticky bit to ensure new files and folders inherit group ownership\nchmod 2775 \"$folder\"\n</code></pre>"}]}